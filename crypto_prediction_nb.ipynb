{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 0: Imports and Setup\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from IPython.display import display\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Configure matplotlib for notebook environment\n",
        "plt.style.use('default')\n",
        "\n",
        "# Import our integrated utilities\n",
        "from src import (\n",
        "    BinanceDataOrganizer, DataConfig, GroupedScaler,\n",
        "    create_lstm_model, evaluate_model,\n",
        "    plot_candlestick_chart,\n",
        "    plot_prediction_accuracy_distribution,\n",
        "    plot_model_performance_summary,\n",
        "    plot_test_performance_metrics,\n",
        "    plot_prediction_candlestick,\n",
        "    production_config, test_config,\n",
        ")\n",
        "\n",
        "print(\"âœ… Imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration Selection\n",
        "\n",
        "Choose between production and test configurations:\n",
        "\n",
        "- **Production Config**: Full-scale deployment with larger models and more data\n",
        "- **Test Config**: Fast execution for development and testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Configuration Selection\n",
        "CONFIG_MODE = 'production'  # Change to 'production' for full-scale deployment\n",
        "\n",
        "if CONFIG_MODE == 'production':\n",
        "    config = production_config\n",
        "    print(\"ðŸš€ PRODUCTION mode\")\n",
        "else:\n",
        "    config = test_config\n",
        "    print(\"âš¡ TEST mode\")\n",
        "\n",
        "print(f\"Config: {config.symbol} {config.timeframe} | {config.start_date} to {config.end_date}\")\n",
        "print(f\"Model: {config.lstm_units} units, {config.epochs} epochs, {config.sequence_length}â†’{config.prediction_length}\")\n",
        "\n",
        "# Create data config from selected configuration\n",
        "data_config = config.get_data_config()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Processing\n",
        "\n",
        "Load cryptocurrency data and create sequences for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Data Loading\n",
        "organizer = BinanceDataOrganizer(data_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Data Scaling\n",
        "scaled_data = organizer.get_scaled_data()\n",
        "X_train_scaled = scaled_data['X_train_scaled']\n",
        "y_train_scaled = scaled_data['y_train_scaled']\n",
        "X_test_scaled = scaled_data['X_test_scaled']\n",
        "y_test_scaled = scaled_data['y_test_scaled']\n",
        "\n",
        "print(f\"âœ… Scaled: {X_train_scaled.shape} | Range: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Creation and Training\n",
        "\n",
        "Create and train the LSTM model using the selected configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Model Creation\n",
        "model = create_lstm_model(\n",
        "    input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]),\n",
        "    lstm_units=config.lstm_units,\n",
        "    dropout_rate=config.dropout_rate,\n",
        "    learning_rate=config.learning_rate,\n",
        "    prediction_length=config.prediction_length\n",
        ")\n",
        "\n",
        "print(f\"âœ… Model: {model.count_params():,} parameters | {config.lstm_units} LSTM units\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Model Training\n",
        "print(f\"ðŸš€ Training {config.epochs} epochs...\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train_scaled,\n",
        "    validation_data=(X_test_scaled, y_test_scaled),\n",
        "    epochs=config.epochs,\n",
        "    batch_size=config.batch_size,\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=config.patience, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=config.lr_patience, factor=config.lr_factor, min_lr=config.min_lr)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"âœ… Training completed\")\n",
        "\n",
        "# Display training history charts\n",
        "print(\"ðŸ“Š Displaying training history...\")\n",
        "plot_model_performance_summary(history, title=f\"{config.symbol} Training History\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Model Evaluation and Analysis\n",
        "# Display final training metrics\n",
        "final_loss = history.history['loss'][-1]\n",
        "final_mae = history.history['mae'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "final_val_mae = history.history['val_mae'][-1]\n",
        "\n",
        "print(f\"ðŸ“Š Final Training Loss: {final_loss:.4f} | MAE: {final_mae:.4f}\")\n",
        "print(f\"ðŸ“Š Final Validation Loss: {final_val_loss:.4f} | MAE: {final_val_mae:.4f}\")\n",
        "\n",
        "# Evaluate model and display test performance metrics\n",
        "evaluation_results = evaluate_model(\n",
        "    model, X_test_scaled, y_test_scaled,\n",
        "    organizer.scaler_y\n",
        ")\n",
        "\n",
        "print(f\"ðŸ“Š Test Performance: MAE={evaluation_results['test_mae']:.2f} | MAPE={evaluation_results['test_mape']:.1f}% | RMSE={evaluation_results['rmse']:.2f}\")\n",
        "\n",
        "# Display test performance metrics and trade-off analysis\n",
        "print(\"ðŸ“Š Displaying test performance metrics...\")\n",
        "plot_test_performance_metrics(evaluation_results, title=f\"{config.symbol} Test Performance Analysis\")\n",
        "\n",
        "# Get predictions for analysis\n",
        "y_pred = evaluation_results['predictions']\n",
        "y_true = evaluation_results['y_true_original']\n",
        "\n",
        "# Prediction analysis\n",
        "prediction_errors = [np.sqrt(np.mean((y_pred[i] - y_true[i]) ** 2)) for i in range(len(y_pred))]\n",
        "best_idx = np.argmin(prediction_errors)\n",
        "worst_idx = np.argmax(prediction_errors)\n",
        "\n",
        "print(f\"ðŸ“Š Best: RMSE={prediction_errors[best_idx]:.2f} | Worst: RMSE={prediction_errors[worst_idx]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Best Prediction Candlestick Chart\n",
        "print(\"ðŸ“ˆ Best Prediction Candlestick Chart:\")\n",
        "# Get input data for the best prediction (last sequence from training data)\n",
        "input_data = X_test_scaled[best_idx]  # This is the input sequence that led to the prediction\n",
        "# Get scaled predictions directly from the model (not unscaled from evaluate_model)\n",
        "pred_data_scaled = model.predict(X_test_scaled[best_idx:best_idx+1], verbose=0)[0]  # Scaled prediction data\n",
        "actual_data_scaled = y_test_scaled[best_idx]  # Scaled actual data\n",
        "plot_prediction_candlestick(pred_data_scaled, actual_data_scaled, input_data,\n",
        "                           f\"Best Prediction (Index {best_idx})\", \n",
        "                           prediction_errors[best_idx])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Worst Prediction Candlestick Chart\n",
        "print(\"ðŸ“ˆ Worst Prediction Candlestick Chart:\")\n",
        "# Get input data for the worst prediction (last sequence from training data)\n",
        "input_data = X_test_scaled[worst_idx]  # This is the input sequence that led to the prediction\n",
        "# Get scaled predictions directly from the model (not unscaled from evaluate_model)\n",
        "pred_data_scaled = model.predict(X_test_scaled[worst_idx:worst_idx+1], verbose=0)[0]  # Scaled prediction data\n",
        "actual_data_scaled = y_test_scaled[worst_idx]  # Scaled actual data\n",
        "plot_prediction_candlestick(pred_data_scaled, actual_data_scaled, input_data,\n",
        "                           f\"Worst Prediction (Index {worst_idx})\", \n",
        "                           prediction_errors[worst_idx])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Random Prediction Candlestick Chart\n",
        "print(\"ðŸ“ˆ Random Prediction Candlestick Chart:\")\n",
        "# Get a random prediction index\n",
        "import random\n",
        "random_idx = random.randint(0, len(y_pred) - 1)\n",
        "print(f\"Random Index: {random_idx}\")\n",
        "\n",
        "# Get input data for the random prediction\n",
        "input_data = X_test_scaled[random_idx]  # This is the input sequence that led to the prediction\n",
        "# Get model predictions in scaled format (same as best/worst charts)\n",
        "pred_data_scaled = model.predict(X_test_scaled[random_idx:random_idx+1], verbose=0)[0]  # Scaled prediction data\n",
        "actual_data_scaled = y_test_scaled[random_idx]  # Scaled actual data\n",
        "plot_prediction_candlestick(pred_data_scaled, actual_data_scaled, input_data,\n",
        "                           f\"Random Prediction (Index {random_idx})\", \n",
        "                           prediction_errors[random_idx])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Summary\n",
        "print(f\"âœ… Complete: {CONFIG_MODE.upper()} mode | {config.symbol} {config.timeframe}\")\n",
        "print(f\"ðŸ“Š Data: {data_summary['total_rows']} rows â†’ {feature_info['total_sequences']} sequences\")\n",
        "print(f\"ðŸ¤– Model: {model.count_params():,} params\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "crypto_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
