{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Crypto Prediction with On-Demand Processing\n",
        "\n",
        "**Process Data Only When Needed - Perfect for 16GB M4 MacBook**\n",
        "\n",
        "This notebook implements on-demand processing where data is processed only when you need it.\n",
        "\n",
        "## Key Features:\n",
        "- **On-Demand Processing**: Process data only when requested\n",
        "- **Memory Efficient**: No data stored in memory until needed\n",
        "- **Flexible**: Process any amount of data on demand\n",
        "- **16GB Compatible**: Designed for M4 MacBook memory constraints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from IPython.display import display\n",
        "import psutil\n",
        "import gc\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Get current memory usage in MB.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "def check_memory_limit(max_memory_mb=3000):\n",
        "    \"\"\"Check if memory usage is within limits.\"\"\"\n",
        "    current_memory = get_memory_usage()\n",
        "    if current_memory > max_memory_mb:\n",
        "        print(f\"‚ö†Ô∏è  Memory usage high: {current_memory:.1f} MB (limit: {max_memory_mb} MB)\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def force_garbage_collection():\n",
        "    \"\"\"Force garbage collection to free memory.\"\"\"\n",
        "    gc.collect()\n",
        "    return get_memory_usage()\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"üíæ Initial memory usage: {get_memory_usage():.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import utilities\n",
        "import sys\n",
        "sys.path.append('/Users/parteekmalik/github/pytorch')\n",
        "\n",
        "from utils import (\n",
        "    get_memory_usage, check_memory_limit, force_garbage_collection,\n",
        "    download_crypto_data, create_minimal_features, create_sliding_windows,\n",
        "    create_lightweight_lstm_model, scale_data, train_model_memory_efficient, evaluate_model\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Utilities imported successfully!\")\n",
        "print(f\"üíæ Initial memory usage: {get_memory_usage():.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# On-Demand Processing Configuration\n",
        "ONDEMAND_CONFIG = {\n",
        "    'MAX_MEMORY_MB': 3000,        # Ultra-safe memory limit\n",
        "    'SEQUENCE_LENGTH': 5,         # Short sequences\n",
        "    'LAG_PERIOD': 3,             # Reduced lag features\n",
        "    'TARGET_COLS': ['Open', 'High', 'Low', 'Close', 'Volume'],\n",
        "    'SYMBOL': 'BTCUSDT',\n",
        "    'INTERVAL': '5m',\n",
        "    'YEAR': '2021',\n",
        "    'MONTHS': ['01', '02', '03', '04', '05', '06'],  # 6 months only\n",
        "    'MAX_SAMPLES': 10000         # Maximum samples to process at once\n",
        "}\n",
        "\n",
        "print(\"‚úÖ On-demand configuration loaded!\")\n",
        "print(f\"üíæ Memory limit: {ONDEMAND_CONFIG['MAX_MEMORY_MB']:,} MB\")\n",
        "print(f\"üîÑ Sequence length: {ONDEMAND_CONFIG['SEQUENCE_LENGTH']}\")\n",
        "print(f\"üìà Lag period: {ONDEMAND_CONFIG['LAG_PERIOD']}\")\n",
        "print(f\"üìä Max samples: {ONDEMAND_CONFIG['MAX_SAMPLES']:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# On-Demand Data Processor\n",
        "class OnDemandProcessor:\n",
        "    \"\"\"Ultra memory-efficient on-demand data processor.\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.scaler_X = None\n",
        "        self.scaler_y = None\n",
        "        self.feature_cols = None\n",
        "        self.is_fitted = False\n",
        "        self.raw_data = None\n",
        "        \n",
        "    def load_data_on_demand(self, symbol, interval, year, months, max_rows=30000):\n",
        "        \"\"\"Load data only when needed.\"\"\"\n",
        "        print(f\"üì• Loading data on-demand (max {max_rows:,} rows)...\")\n",
        "        \n",
        "        all_data = []\n",
        "        total_rows = 0\n",
        "        \n",
        "        for month in months:\n",
        "            if total_rows >= max_rows:\n",
        "                print(f\"‚ö†Ô∏è  Reached row limit at {total_rows:,} rows\")\n",
        "                break\n",
        "            \n",
        "            # Try Binance Vision first (more reliable), then API\n",
        "            df = download_binance_vision_data(symbol, interval, year, month)\n",
        "            if df is None:\n",
        "                print(f\"   Trying API fallback for {year}-{month}...\")\n",
        "                df = download_binance_klines_data(symbol, interval, year, month)\n",
        "            \n",
        "            if df is not None:\n",
        "                # Limit rows if approaching limit\n",
        "                remaining_rows = max_rows - total_rows\n",
        "                if len(df) > remaining_rows:\n",
        "                    df = df.head(remaining_rows)\n",
        "                    print(f\"   Truncated to {len(df)} rows to stay within memory limit\")\n",
        "                \n",
        "                all_data.append(df)\n",
        "                total_rows += len(df)\n",
        "                \n",
        "                current_memory = get_memory_usage()\n",
        "                print(f\"   Total rows: {total_rows:,}, Memory: {current_memory:.1f} MB\")\n",
        "                \n",
        "                # Check memory limit\n",
        "                if not check_memory_limit(self.config['MAX_MEMORY_MB']):\n",
        "                    print(f\"‚ö†Ô∏è  Memory limit reached, stopping download\")\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"   Failed to download {year}-{month}\")\n",
        "        \n",
        "        if all_data:\n",
        "            self.raw_data = pd.concat(all_data, ignore_index=True)\n",
        "            print(f\"‚úÖ Data loaded on-demand!\")\n",
        "            print(f\"   Final shape: {self.raw_data.shape}\")\n",
        "            print(f\"   Memory usage: {get_memory_usage():.1f} MB\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"‚ùå No data loaded\")\n",
        "            return False\n",
        "    \n",
        "    def process_data_on_demand(self, max_samples=None):\n",
        "        \"\"\"Process data only when requested.\"\"\"\n",
        "        if self.raw_data is None:\n",
        "            print(\"‚ùå No data loaded. Call load_data_on_demand() first.\")\n",
        "            return None, None, None\n",
        "        \n",
        "        max_samples = max_samples or self.config['MAX_SAMPLES']\n",
        "        print(f\"üîÑ Processing data on-demand (max {max_samples:,} samples)...\")\n",
        "        \n",
        "        # Create features\n",
        "        features = create_minimal_features(self.raw_data, self.config['LAG_PERIOD'])\n",
        "        features_clean = features.dropna()\n",
        "        \n",
        "        if len(features_clean) < self.config['SEQUENCE_LENGTH']:\n",
        "            print(\"‚ùå Not enough data for processing\")\n",
        "            return None, None, None\n",
        "        \n",
        "        # Get feature columns\n",
        "        self.feature_cols = [col for col in features_clean.columns \n",
        "                           if col not in ['Open time', 'Close time'] + self.config['TARGET_COLS']]\n",
        "        \n",
        "        # Create sliding windows\n",
        "        X, y, _ = create_sliding_windows(\n",
        "            features_clean, \n",
        "            self.config['SEQUENCE_LENGTH'], \n",
        "            self.config['TARGET_COLS']\n",
        "        )\n",
        "        \n",
        "        if len(X) == 0:\n",
        "            print(\"‚ùå No valid samples created\")\n",
        "            return None, None, None\n",
        "        \n",
        "        # Limit samples if needed\n",
        "        if len(X) > max_samples:\n",
        "            X = X[:max_samples]\n",
        "            y = y[:max_samples]\n",
        "            print(f\"   Limited to {max_samples:,} samples for memory efficiency\")\n",
        "        \n",
        "        print(f\"‚úÖ On-demand processing completed!\")\n",
        "        print(f\"   Samples: {len(X):,}\")\n",
        "        print(f\"   Features: {len(self.feature_cols)}\")\n",
        "        print(f\"   Memory: {get_memory_usage():.1f} MB\")\n",
        "        \n",
        "        return X, y, self.feature_cols\n",
        "    \n",
        "    def fit_scalers_on_demand(self, X_sample, y_sample):\n",
        "        \"\"\"Fit scalers only when needed.\"\"\"\n",
        "        from sklearn.preprocessing import MinMaxScaler\n",
        "        \n",
        "        print(\"üî¢ Fitting scalers on-demand...\")\n",
        "        \n",
        "        self.scaler_X = MinMaxScaler()\n",
        "        self.scaler_y = MinMaxScaler()\n",
        "        \n",
        "        X_reshaped = X_sample.reshape(-1, X_sample.shape[-1])\n",
        "        self.scaler_X.fit(X_reshaped)\n",
        "        self.scaler_y.fit(y_sample)\n",
        "        \n",
        "        self.is_fitted = True\n",
        "        print(f\"‚úÖ Scalers fitted on {len(X_sample)} samples\")\n",
        "    \n",
        "    def train_model_on_demand(self, X, y, epochs=20, batch_size=32):\n",
        "        \"\"\"Train model only when requested.\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            print(\"‚ùå Scalers not fitted. Call fit_scalers_on_demand() first.\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"üöÄ Training model on-demand...\")\n",
        "        \n",
        "        # Split data\n",
        "        split_idx = int(len(X) * 0.8)\n",
        "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "        \n",
        "        # Scale data\n",
        "        X_train_scaled, y_train_scaled, _, _ = scale_data(X_train, y_train, fit_scalers=True)\n",
        "        X_test_scaled, y_test_scaled = scale_data(X_test, y_test, self.scaler_X, self.scaler_y, fit_scalers=False)\n",
        "        \n",
        "        # Create model\n",
        "        input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
        "        output_dim = y_train_scaled.shape[1]\n",
        "        \n",
        "        model = create_lightweight_lstm_model(input_shape, output_dim)\n",
        "        \n",
        "        # Train model\n",
        "        history = train_model_memory_efficient(\n",
        "            model, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled,\n",
        "            epochs=epochs, batch_size=batch_size, verbose=1\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úÖ Model training completed!\")\n",
        "        print(f\"   Final loss: {history.history['loss'][-1]:.4f}\")\n",
        "        print(f\"   Memory usage: {get_memory_usage():.1f} MB\")\n",
        "        \n",
        "        return model, history, X_test_scaled, y_test_scaled\n",
        "\n",
        "print(\"‚úÖ On-demand processor loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# On-Demand Data Loading\n",
        "print(\"üì• ON-DEMAND DATA LOADING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create on-demand processor\n",
        "processor = OnDemandProcessor(ONDEMAND_CONFIG)\n",
        "\n",
        "# Load data only when needed using common download function\n",
        "success = processor.load_data_on_demand(\n",
        "    ONDEMAND_CONFIG['SYMBOL'],\n",
        "    ONDEMAND_CONFIG['INTERVAL'],\n",
        "    f\"{ONDEMAND_CONFIG['YEAR']} {ONDEMAND_CONFIG['MONTHS'][0]}\",\n",
        "    f\"{ONDEMAND_CONFIG['YEAR']} {ONDEMAND_CONFIG['MONTHS'][-1]}\",\n",
        "    max_rows=30000  # Ultra-safe limit\n",
        ")\n",
        "\n",
        "if success:\n",
        "    print(f\"\\n‚úÖ Data loaded successfully!\")\n",
        "    print(f\"üìä Data shape: {processor.raw_data.shape}\")\n",
        "    print(f\"üìÖ Date range: {processor.raw_data['Open time'].min()} to {processor.raw_data['Open time'].max()}\")\n",
        "    print(f\"üíæ Memory usage: {get_memory_usage():.1f} MB\")\n",
        "    \n",
        "    # Show sample data\n",
        "    print(f\"\\nüìã Sample Data:\")\n",
        "    display(processor.raw_data.head(3))\n",
        "    \n",
        "    # Check memory\n",
        "    if not check_memory_limit(ONDEMAND_CONFIG['MAX_MEMORY_MB']):\n",
        "        print(\"‚ö†Ô∏è  Memory usage high after data loading!\")\n",
        "        print(\"üí° Consider reducing data size\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Data loading failed\")\n",
        "    processor = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# On-Demand Data Processing\n",
        "if processor is not None:\n",
        "    print(\"üîÑ ON-DEMAND DATA PROCESSING\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Process data only when needed\n",
        "    X, y, feature_cols = processor.process_data_on_demand(max_samples=5000)  # Process 5k samples\n",
        "    \n",
        "    if X is not None:\n",
        "        print(f\"\\n‚úÖ Data processing completed!\")\n",
        "        print(f\"üìä Processed samples: {len(X):,}\")\n",
        "        print(f\"üìà Features: {len(feature_cols)}\")\n",
        "        print(f\"üíæ Memory usage: {get_memory_usage():.1f} MB\")\n",
        "        \n",
        "        # Show sample data\n",
        "        print(f\"\\nüìã Sample Processed Data:\")\n",
        "        print(f\"   X shape: {X.shape}\")\n",
        "        print(f\"   y shape: {y.shape}\")\n",
        "        print(f\"   Feature columns: {feature_cols[:10]}...\")\n",
        "        \n",
        "        # Check memory\n",
        "        if not check_memory_limit(ONDEMAND_CONFIG['MAX_MEMORY_MB']):\n",
        "            print(\"‚ö†Ô∏è  Memory usage high after processing!\")\n",
        "            print(\"üí° Consider reducing sample size\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ùå Data processing failed\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå Processor not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# On-Demand Model Training\n",
        "if 'X' in locals() and X is not None:\n",
        "    print(\"üöÄ ON-DEMAND MODEL TRAINING\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Fit scalers on sample data\n",
        "    sample_size = min(1000, len(X))\n",
        "    processor.fit_scalers_on_demand(X[:sample_size], y[:sample_size])\n",
        "    \n",
        "    # Train model only when needed\n",
        "    model, history, X_test_scaled, y_test_scaled = processor.train_model_on_demand(\n",
        "        X, y, epochs=15, batch_size=32  # Reduced for demo\n",
        "    )\n",
        "    \n",
        "    if model is not None:\n",
        "        print(f\"\\n‚úÖ Model training completed!\")\n",
        "        print(f\"üìä Model architecture:\")\n",
        "        print(f\"   Input shape: {X.shape[1:]}\")\n",
        "        print(f\"   Output shape: {y.shape[1]}\")\n",
        "        print(f\"   Parameters: {model.count_params():,}\")\n",
        "        print(f\"üíæ Memory usage: {get_memory_usage():.1f} MB\")\n",
        "        \n",
        "        # Show training history\n",
        "        print(f\"\\nüìà Training History:\")\n",
        "        print(f\"   Final loss: {history.history['loss'][-1]:.4f}\")\n",
        "        print(f\"   Final val_loss: {history.history['val_loss'][-1]:.4f}\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ùå Model training failed\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå No processed data available for training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# On-Demand Model Evaluation\n",
        "if 'model' in locals() and model is not None:\n",
        "    print(\"üìä ON-DEMAND MODEL EVALUATION\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Evaluate model\n",
        "    results = evaluate_model(model, X_test_scaled, y_test_scaled, processor.scaler_y)\n",
        "    \n",
        "    # Show sample predictions\n",
        "    print(f\"\\nüìã Sample Predictions (First 3):\")\n",
        "    for i in range(min(3, len(results['y_pred']))):\n",
        "        print(f\"   Sample {i+1}:\")\n",
        "        print(f\"     Actual:   {results['y_test'][i]}\")\n",
        "        print(f\"     Predicted: {results['y_pred'][i]}\")\n",
        "        print(f\"     Error:    {np.abs(results['y_test'][i] - results['y_pred'][i])}\")\n",
        "    \n",
        "    # Memory usage summary\n",
        "    print(f\"\\nüíæ Memory Usage Summary:\")\n",
        "    print(f\"   Current memory: {get_memory_usage():.1f} MB\")\n",
        "    \n",
        "    if get_memory_usage() < ONDEMAND_CONFIG['MAX_MEMORY_MB']:\n",
        "        print(f\"‚úÖ Memory usage within limits!\")\n",
        "        print(f\"   On-demand approach successful for 16GB M4 MacBook\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Memory usage high\")\n",
        "        print(f\"   Consider reducing sample size or data\")\n",
        "    \n",
        "    print(f\"\\nüéâ On-demand approach completed successfully!\")\n",
        "    print(f\"   Perfect for flexible processing on 16GB M4 MacBook\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Model not available for evaluation\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
