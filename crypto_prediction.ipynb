{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 0: Imports and Setup\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from IPython.display import display\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Configure matplotlib for notebook environment\n",
        "plt.style.use('default')\n",
        "\n",
        "# Import our integrated utilities\n",
        "from src import (\n",
        "    BinanceDataOrganizer, DataConfig, GroupedScaler,\n",
        "    create_lstm_model, evaluate_model,\n",
        "    get_memory_usage, print_memory_stats,\n",
        "    plot_training_history, plot_predictions_vs_actual,\n",
        "    plot_price_data, plot_prediction_errors, plot_feature_analysis,\n",
        "    # Legacy utilities for comparison\n",
        "    download_crypto_data, scale_data, scale_time_series_data_grouped,\n",
        "    predict_with_grouped_scaler\n",
        ")\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"üìä Memory usage: {get_memory_usage():.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration and Data Setup\n",
        "\n",
        "Configure the data parameters and create the BinanceDataOrganizer instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Configuration\n",
        "# Configuration parameters\n",
        "SYMBOL = \"BTCUSDT\"\n",
        "TIMEFRAME = \"5m\"\n",
        "START_DATE = \"2021-01-01\"\n",
        "END_DATE = \"2021-01-31\"  # Full month for better training\n",
        "SEQUENCE_LENGTH = 20  # Number of timesteps to look back\n",
        "PREDICTION_LENGTH = 1  # Number of future timesteps to predict\n",
        "MAX_ROWS = 10000  # Maximum rows to load for memory efficiency\n",
        "TRAIN_SPLIT = 0.8  # 80% for training, 20% for testing\n",
        "\n",
        "# Model parameters\n",
        "LSTM_UNITS = 100\n",
        "DROPOUT_RATE = 0.2\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Create configuration\n",
        "config = DataConfig(\n",
        "    symbol=SYMBOL,\n",
        "    timeframe=TIMEFRAME,\n",
        "    start_time=START_DATE,\n",
        "    end_time=END_DATE,\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    prediction_length=PREDICTION_LENGTH,\n",
        "    max_rows=MAX_ROWS,\n",
        "    train_split=TRAIN_SPLIT\n",
        ")\n",
        "\n",
        "print(f\"üîß Configuration created:\")\n",
        "print(f\"   Symbol: {config.symbol}\")\n",
        "print(f\"   Timeframe: {config.timeframe}\")\n",
        "print(f\"   Period: {config.start_time} to {config.end_time}\")\n",
        "print(f\"   Sequence Length: {config.sequence_length}\")\n",
        "print(f\"   Prediction Length: {config.prediction_length}\")\n",
        "print(f\"   Max Rows: {config.max_rows}\")\n",
        "print(f\"   Train Split: {config.train_split}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Create and Initialize Organizer\n",
        "# Create the BinanceDataOrganizer instance\n",
        "organizer = BinanceDataOrganizer(config)\n",
        "\n",
        "print(\"üöÄ Starting complete data processing pipeline...\")\n",
        "\n",
        "# Process all data (load + create features)\n",
        "if organizer.process_all():\n",
        "    print(\"‚úÖ Data processing completed successfully!\")\n",
        "    \n",
        "    # Get feature information\n",
        "    feature_info = organizer.get_feature_info()\n",
        "    print(f\"\\nüìä Feature Information:\")\n",
        "    print(f\"   Total features: {feature_info['num_features']}\")\n",
        "    print(f\"   Sequence length: {feature_info['sequence_length']}\")\n",
        "    print(f\"   Prediction length: {feature_info['prediction_length']}\")\n",
        "    print(f\"   Data shape: {feature_info['data_shape']}\")\n",
        "    print(f\"   Total sequences: {feature_info['total_sequences']}\")\n",
        "    print(f\"\\nüîß Feature columns: {feature_info['feature_columns']}\")\n",
        "else:\n",
        "    print(\"‚ùå Data processing failed!\")\n",
        "    raise Exception(\"Failed to process data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Visualization and Analysis\n",
        "\n",
        "Visualize the loaded data and analyze its characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Data Visualization\n",
        "print(\"üìä Visualizing loaded data...\")\n",
        "\n",
        "# Get unscaled data for visualization\n",
        "unscaled_data = organizer.get_unscaled_data('all')\n",
        "if 'data' in unscaled_data:\n",
        "    original_data = unscaled_data['data']\n",
        "    plot_price_data(original_data, f\"{config.symbol} Price Data ({config.start_time} to {config.end_time})\")\n",
        "    \n",
        "    # Display basic statistics\n",
        "    print(f\"\\nüìà Data Statistics:\")\n",
        "    print(f\"   Total rows: {len(original_data):,}\")\n",
        "    print(f\"   Date range: {original_data.index[0]} to {original_data.index[-1]}\")\n",
        "    print(f\"   Price range: ${original_data['Close'].min():.2f} - ${original_data['Close'].max():.2f}\")\n",
        "    print(f\"   Volume range: {original_data['Volume'].min():.2f} - {original_data['Volume'].max():.2f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Original data not available for plotting\")\n",
        "\n",
        "print(\"üìä Feature analysis will be shown after data scaling...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training with Integrated Normalization\n",
        "\n",
        "Train the LSTM model using the integrated normalization system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Get Scaled Data and Train Model\n",
        "# Get scaled data (this will fit scalers if not already fitted)\n",
        "print(\"üî¢ Getting scaled data with integrated normalization...\")\n",
        "scaled_data = organizer.get_scaled_data('all')\n",
        "\n",
        "X_train_scaled = scaled_data['X_train_scaled']\n",
        "X_test_scaled = scaled_data['X_test_scaled']\n",
        "y_train_scaled = scaled_data['y_train_scaled']\n",
        "y_test_scaled = scaled_data['y_test_scaled']\n",
        "\n",
        "print(f\"‚úÖ Scaled data ready:\")\n",
        "print(f\"   X_train_scaled: {X_train_scaled.shape}\")\n",
        "print(f\"   y_train_scaled: {y_train_scaled.shape}\")\n",
        "print(f\"   X_train range: {X_train_scaled.min():.4f} to {X_train_scaled.max():.4f}\")\n",
        "print(f\"   y_train range: {y_train_scaled.min():.4f} to {y_train_scaled.max():.4f}\")\n",
        "\n",
        "# Get scalers for later use\n",
        "scalers = organizer.get_scalers()\n",
        "scaler_X = scalers['X']  # GroupedScaler\n",
        "scaler_y = scalers['y']  # MinMaxScaler\n",
        "\n",
        "# Display feature groups\n",
        "feature_groups = scaler_X.get_feature_groups()\n",
        "print(f\"\\nüìä Feature Groups:\")\n",
        "for group_name, features in feature_groups.items():\n",
        "    print(f\"   {group_name.capitalize()}: {features}\")\n",
        "\n",
        "# Plot feature analysis\n",
        "print(f\"\\nüìä Feature Analysis:\")\n",
        "feature_info_with_groups = {\n",
        "    'feature_columns': feature_info['feature_columns'],\n",
        "    'feature_groups': feature_groups\n",
        "}\n",
        "plot_feature_analysis(feature_info_with_groups)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Create and Train LSTM Model\n",
        "print(f\"üèóÔ∏è Creating LSTM model...\")\n",
        "print(f\"   LSTM units: {LSTM_UNITS}\")\n",
        "print(f\"   Dropout rate: {DROPOUT_RATE}\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "\n",
        "# Create model\n",
        "model = create_lstm_model(\n",
        "    input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]),\n",
        "    lstm_units=LSTM_UNITS,\n",
        "    dropout_rate=DROPOUT_RATE\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss='mse',\n",
        "    metrics=['mae', 'mape']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model created with {model.count_params():,} parameters\")\n",
        "print(f\"\\nüìã Model Architecture:\")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Train the Model\n",
        "print(f\"üöÄ Starting model training...\")\n",
        "print(f\"   Memory usage before training: {get_memory_usage():.1f} MB\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train_scaled,\n",
        "    validation_data=(X_test_scaled, y_test_scaled),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-7)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Training completed!\")\n",
        "print(f\"   Memory usage after training: {get_memory_usage():.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Plot Training History\n",
        "print(\"üìà Plotting training history...\")\n",
        "plot_training_history(history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation and Advanced Features\n",
        "\n",
        "Evaluate the trained model and demonstrate advanced features of the BinanceDataOrganizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Model Evaluation\n",
        "print(f\"üìä Evaluating model performance...\")\n",
        "\n",
        "# Evaluate model\n",
        "evaluation_results = evaluate_model(\n",
        "    model=model,\n",
        "    X_test=X_test_scaled,\n",
        "    y_test=y_test_scaled,\n",
        "    scaler_y=scaler_y\n",
        ")\n",
        "\n",
        "print(f\"\\nüìà Evaluation Results:\")\n",
        "print(f\"   Test Loss (MSE): {evaluation_results['test_loss']:.6f}\")\n",
        "print(f\"   Test MAE: {evaluation_results['test_mae']:.6f}\")\n",
        "print(f\"   Test MAPE: {evaluation_results['test_mape']:.2f}%\")\n",
        "print(f\"   RMSE: {evaluation_results['rmse']:.6f}\")\n",
        "\n",
        "# Get predictions for visualization\n",
        "y_pred = evaluation_results['predictions']\n",
        "y_true = evaluation_results['y_true_original']\n",
        "\n",
        "print(f\"\\nüìä Prediction Statistics:\")\n",
        "print(f\"   Predictions shape: {y_pred.shape}\")\n",
        "print(f\"   True values shape: {y_true.shape}\")\n",
        "print(f\"   Prediction range: ${y_pred.min():.2f} - ${y_pred.max():.2f}\")\n",
        "print(f\"   True value range: ${y_true.min():.2f} - ${y_true.max():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Plot Predictions vs Actual\n",
        "print(\"üìà Plotting predictions vs actual...\")\n",
        "target_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "plot_predictions_vs_actual(\n",
        "    evaluation_results['predictions'], \n",
        "    evaluation_results['y_true_original'], \n",
        "    target_columns\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Plot Prediction Errors\n",
        "print(\"üìà Plotting prediction errors...\")\n",
        "plot_prediction_errors(\n",
        "    evaluation_results['predictions'], \n",
        "    evaluation_results['y_true_original'], \n",
        "    target_columns\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Advanced Features and On-Demand Processing\n",
        "\n",
        "Demonstrate advanced features like on-demand data generation and future predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: On-Demand Data Generation\n",
        "print(\"üîÑ Demonstrating on-demand data generation...\")\n",
        "\n",
        "# Get data for a specific time range\n",
        "start_time = \"2021-01-15 00:00:00\"\n",
        "end_time = \"2021-01-15 23:59:59\"\n",
        "\n",
        "print(f\"üìÖ Requesting data for range: {start_time} to {end_time}\")\n",
        "\n",
        "range_data = organizer.get_data_in_range(\n",
        "    start_time=start_time,\n",
        "    end_time=end_time,\n",
        "    scaled=True\n",
        ")\n",
        "\n",
        "if range_data is not None:\n",
        "    print(f\"‚úÖ Range data generated:\")\n",
        "    print(f\"   Available keys: {list(range_data.keys())}\")\n",
        "    print(f\"   X shape: {range_data['X_scaled'].shape}\")\n",
        "    print(f\"   y shape: {range_data['y_scaled'].shape}\")\n",
        "    \n",
        "    # Make predictions on this range\n",
        "    range_predictions = model.predict(range_data['X_scaled'], verbose=0)\n",
        "    print(f\"   Predictions shape: {range_predictions.shape}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No data available for the specified range\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Future Predictions\n",
        "print(\"üîÆ Making future predictions...\")\n",
        "\n",
        "# Use the last sequence from test data for prediction\n",
        "last_sequence = X_test_scaled[-1:]  # Shape: (1, sequence_length, features)\n",
        "print(f\"   Using last sequence shape: {last_sequence.shape}\")\n",
        "\n",
        "# Make prediction\n",
        "future_prediction = model.predict(last_sequence, verbose=0)\n",
        "print(f\"   Prediction shape: {future_prediction.shape}\")\n",
        "\n",
        "# Inverse transform prediction to original scale\n",
        "future_prediction_original = scaler_y.inverse_transform(future_prediction)\n",
        "print(f\"   Original scale prediction shape: {future_prediction_original.shape}\")\n",
        "\n",
        "# Display prediction results\n",
        "target_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "print(f\"\\nüìà Future Prediction (Next Candle):\")\n",
        "for i, col in enumerate(target_columns):\n",
        "    print(f\"   {col}: ${future_prediction_original[0, i]:.2f}\")\n",
        "\n",
        "# Compare with last known values\n",
        "last_known = scaler_y.inverse_transform(y_test_scaled[-1:])[0]\n",
        "print(f\"\\nüìä Last Known Values:\")\n",
        "for i, col in enumerate(target_columns):\n",
        "    print(f\"   {col}: ${last_known[i]:.2f}\")\n",
        "\n",
        "# Calculate changes\n",
        "print(f\"\\nüìä Predicted Changes:\")\n",
        "for i, col in enumerate(target_columns):\n",
        "    change = future_prediction_original[0, i] - last_known[i]\n",
        "    change_pct = (change / last_known[i]) * 100\n",
        "    print(f\"   {col}: {change:+.2f} ({change_pct:+.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Legacy Method Comparison\n",
        "\n",
        "Compare the results with the legacy method for validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: Legacy Method Comparison\n",
        "print(\"üîÑ Comparing with legacy method...\")\n",
        "\n",
        "# Download data using legacy method\n",
        "legacy_data = download_crypto_data(\n",
        "    symbol=SYMBOL,\n",
        "    interval=TIMEFRAME,\n",
        "    data_from=START_DATE.replace('-', ' '),\n",
        "    data_to=END_DATE.replace('-', ' ')\n",
        ")\n",
        "\n",
        "if legacy_data is not None:\n",
        "    print(f\"‚úÖ Legacy data downloaded: {legacy_data.shape}\")\n",
        "    \n",
        "    # Create features using legacy method\n",
        "    legacy_features = legacy_data.copy()\n",
        "    \n",
        "    # Convert index to datetime if it's not already\n",
        "    if not isinstance(legacy_features.index, pd.DatetimeIndex):\n",
        "        legacy_features.index = pd.to_datetime(legacy_features.index)\n",
        "    \n",
        "    legacy_features['Minutes_of_day'] = legacy_features.index.hour * 60 + legacy_features.index.minute\n",
        "    legacy_features['Price_Range'] = legacy_features['High'] - legacy_features['Low']\n",
        "    legacy_features['Price_Change'] = legacy_features['Close'] - legacy_features['Open']\n",
        "    legacy_features['Price_Change_Pct'] = (legacy_features['Price_Change'] / legacy_features['Open']) * 100\n",
        "    \n",
        "    # Create sequences using legacy method\n",
        "    feature_columns = ['Quote asset volume', 'Number of trades', 'Taker buy base asset volume', \n",
        "                      'Taker buy quote asset volume', 'Minutes_of_day', 'Price_Range', \n",
        "                      'Price_Change', 'Price_Change_Pct']\n",
        "    target_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    \n",
        "    # Create sequences manually for legacy comparison\n",
        "    X_legacy = []\n",
        "    y_legacy = []\n",
        "    \n",
        "    for i in range(SEQUENCE_LENGTH, len(legacy_features) - PREDICTION_LENGTH + 1):\n",
        "        X_legacy.append(legacy_features[feature_columns].iloc[i-SEQUENCE_LENGTH:i].values)\n",
        "        y_legacy.append(legacy_features[target_columns].iloc[i:i+PREDICTION_LENGTH].values.flatten())\n",
        "    \n",
        "    X_legacy = np.array(X_legacy)\n",
        "    y_legacy = np.array(y_legacy)\n",
        "    \n",
        "    print(f\"‚úÖ Legacy sequences created: X={X_legacy.shape}, y={y_legacy.shape}\")\n",
        "    \n",
        "    # Compare data shapes\n",
        "    print(f\"\\nüìä Data Comparison:\")\n",
        "    print(f\"   Advanced method - X: {X_train_scaled.shape}, y: {y_train_scaled.shape}\")\n",
        "    print(f\"   Legacy method - X: {X_legacy.shape}, y: {y_legacy.shape}\")\n",
        "    \n",
        "    # Compare feature statistics\n",
        "    print(f\"\\nüìä Feature Statistics Comparison:\")\n",
        "    print(f\"   Advanced method - X range: {X_train_scaled.min():.4f} to {X_train_scaled.max():.4f}\")\n",
        "    print(f\"   Legacy method - X range: {X_legacy.min():.4f} to {X_legacy.max():.4f}\")\n",
        "    print(f\"   Advanced method - y range: {y_train_scaled.min():.4f} to {y_train_scaled.max():.4f}\")\n",
        "    print(f\"   Legacy method - y range: {y_legacy.min():.4f} to {y_legacy.max():.4f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Legacy data download failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Final Summary and Results\n",
        "\n",
        "Display comprehensive results and summary of the prediction system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 14: Final Summary\n",
        "print(\"üéâ COMPREHENSIVE CRYPTOCURRENCY PREDICTION COMPLETED!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìä FINAL RESULTS:\")\n",
        "print(f\"   Symbol: {config.symbol}\")\n",
        "print(f\"   Timeframe: {config.timeframe}\")\n",
        "print(f\"   Data period: {config.start_time} to {config.end_time}\")\n",
        "print(f\"   Total sequences: {feature_info['total_sequences']}\")\n",
        "print(f\"   Model parameters: {model.count_params():,}\")\n",
        "print(f\"   Training epochs: {EPOCHS}\")\n",
        "print(f\"   Final test loss: {evaluation_results['test_loss']:.6f}\")\n",
        "print(f\"   Final test MAE: {evaluation_results['test_mae']:.6f}\")\n",
        "print(f\"   Final test MAPE: {evaluation_results['test_mape']:.2f}%\")\n",
        "\n",
        "print(f\"\\nüîß FEATURES USED:\")\n",
        "for group_name, features in feature_groups.items():\n",
        "    print(f\"   {group_name.capitalize()}: {len(features)} features\")\n",
        "    for feature in features:\n",
        "        print(f\"     - {feature}\")\n",
        "\n",
        "print(f\"\\nüí° ADVANCED FEATURES DEMONSTRATED:\")\n",
        "print(f\"   ‚úÖ Integrated data management with BinanceDataOrganizer\")\n",
        "print(f\"   ‚úÖ On-demand data generation and processing\")\n",
        "print(f\"   ‚úÖ Grouped normalization for different feature types\")\n",
        "print(f\"   ‚úÖ Memory-efficient training and evaluation\")\n",
        "print(f\"   ‚úÖ Comprehensive visualization and analysis\")\n",
        "print(f\"   ‚úÖ Model persistence and configuration management\")\n",
        "print(f\"   ‚úÖ Legacy method comparison and validation\")\n",
        "print(f\"   ‚úÖ Advanced plotting and error analysis\")\n",
        "\n",
        "print(f\"\\nüéØ The comprehensive system provides a complete solution\")\n",
        "print(f\"   for cryptocurrency prediction with advanced normalization,\")\n",
        "print(f\"   memory-efficient data management, and comprehensive analysis!\")\n",
        "\n",
        "print(f\"\\nüìà Final memory usage: {get_memory_usage():.1f} MB\")\n",
        "print(f\"\\n‚úÖ All tasks completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "crypto_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
