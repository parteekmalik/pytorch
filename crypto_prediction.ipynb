{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cryptocurrency Price Prediction with LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from utils import download_crypto_data, scale_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Global configuration loaded successfully!\n",
      "üìä Data: BTCUSDT 5m from 2021 01 to 2021 02\n",
      "üß† Model: 50 units, 50 epochs, batch size 32\n",
      "üîß Features: 5 lag period, 10 candle sequence, no rolling windows\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# GLOBAL CONFIGURATION PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Data Configuration\n",
    "SYMBOL = \"BTCUSDT\"                    # Trading pair symbol\n",
    "INTERVAL = \"5m\"                       # Time interval (1m, 5m, 15m, 1h, 4h, 1d)\n",
    "DATA_FROM = \"2021 01\"                 # Start date for data download (YYYY MM)\n",
    "DATA_TO = \"2021 02\"                   # End date for data download (YYYY MM)\n",
    "\n",
    "# Model Configuration\n",
    "LSTM_UNITS = 50                       # Number of LSTM units\n",
    "DROPOUT_RATE = 0.2                    # Dropout rate for regularization\n",
    "EPOCHS = 50                           # Number of training epochs\n",
    "BATCH_SIZE = 32                       # Batch size for training\n",
    "TEST_SIZE = 0.2                       # Proportion of data for testing (0.2 = 20%)\n",
    "\n",
    "# Feature Engineering Configuration\n",
    "LAG_PERIOD = 5                        # Single lag period for all columns\n",
    "SEQUENCE_LENGTH = 10                  # Number of previous candles to use for prediction\n",
    "\n",
    "# Prediction Configuration\n",
    "PREDICTION_LENGTH = 1                 # Number of future steps to predict\n",
    "SAMPLE_SIZE = 200                     # Number of samples for demonstration\n",
    "\n",
    "# Visualization Configuration\n",
    "MAX_POINTS = 2000                     # Maximum points to plot for performance\n",
    "MAX_SAMPLES = 1000                    # Maximum samples for prediction plots\n",
    "\n",
    "print(\"‚úÖ Global configuration loaded successfully!\")\n",
    "print(f\"üìä Data: {SYMBOL} {INTERVAL} from {DATA_FROM} to {DATA_TO}\")\n",
    "print(f\"üß† Model: {LSTM_UNITS} units, {EPOCHS} epochs, batch size {BATCH_SIZE}\")\n",
    "print(f\"üîß Features: {LAG_PERIOD} lag period, {SEQUENCE_LENGTH} candle sequence, no rolling windows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "This section contains functions to download and load cryptocurrency data from Binance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_range(from_date, to_date):\n",
    "    \"\"\"\n",
    "    Generate years and months from date range.\n",
    "    \n",
    "    Args:\n",
    "        from_date (str): Start date in \"YYYY MM\" format\n",
    "        to_date (str): End date in \"YYYY MM\" format\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (years, months) - lists of years and months to download\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    start_year, start_month = from_date.split()\n",
    "    end_year, end_month = to_date.split()\n",
    "    \n",
    "    start_year = int(start_year)\n",
    "    start_month = int(start_month)\n",
    "    end_year = int(end_year)\n",
    "    end_month = int(end_month)\n",
    "    \n",
    "    years = []\n",
    "    months = []\n",
    "    \n",
    "    current_year = start_year\n",
    "    current_month = start_month\n",
    "    \n",
    "    while (current_year < end_year) or (current_year == end_year and current_month <= end_month):\n",
    "        year_str = str(current_year)\n",
    "        month_str = f\"{current_month:02d}\"\n",
    "        \n",
    "        if year_str not in years:\n",
    "            years.append(year_str)\n",
    "        if month_str not in months:\n",
    "            months.append(month_str)\n",
    "        \n",
    "        # Move to next month\n",
    "        if current_month == 12:\n",
    "            current_year += 1\n",
    "            current_month = 1\n",
    "        else:\n",
    "            current_month += 1\n",
    "    \n",
    "    return years, months\n",
    "\n",
    "\n",
    "def load_data_by_date_range(symbol, interval, from_date, to_date):\n",
    "    \"\"\"\n",
    "    Load data for a specific date range, combining into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): Trading pair symbol\n",
    "        interval (str): Time interval\n",
    "        from_date (str): Start date in \"YYYY MM\" format\n",
    "        to_date (str): End date in \"YYYY MM\" format\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with all data in the date range\n",
    "    \"\"\"\n",
    "    years, months = generate_date_range(from_date, to_date)\n",
    "    print(f\"Loading data from {from_date} to {to_date}\")\n",
    "    print(f\"Years: {years}\")\n",
    "    print(f\"Months: {months}\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for year in years:\n",
    "        print(f\"Loading data for year {year}...\")\n",
    "        year_data = load_multiple_months_data(symbol, interval, year, months)\n",
    "        if year_data is not None:\n",
    "            all_data.append(year_data)\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        combined_df = combined_df.sort_values('Open time').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Total combined data shape: {combined_df.shape}\")\n",
    "        print(f\"Date range: {combined_df['Open time'].min()} to {combined_df['Open time'].max()}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No data loaded successfully\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions removed - using common download function from utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• DOWNLOADING CRYPTOCURRENCY DATA\n",
      "   Symbol: BTCUSDT\n",
      "   Interval: 5m\n",
      "   Date range: 2021 01 to 2021 02\n",
      "   Max rows: 50,000\n",
      "==================================================\n",
      "üìÖ Date range: 2021 months 1 to 2\n",
      "üìä Will download months: ['01', '02']\n",
      "üîÑ Loading data with memory efficiency (max 50,000 rows)...\n",
      "üì• Downloading BTCUSDT 5m data for 2021-01 from Binance Vision...\n",
      "‚úÖ Downloaded 8928 rows for 2021-01\n",
      "   Total rows: 8,928\n",
      "üì• Downloading BTCUSDT 5m data for 2021-02 from Binance Vision...\n",
      "‚úÖ Downloaded 8049 rows for 2021-02\n",
      "   Total rows: 16,977\n",
      "‚úÖ Data loading completed!\n",
      "   Final shape: (16977, 12)\n",
      "\\n‚úÖ Download completed successfully!\n",
      "üìä Final data shape: (16977, 12)\n",
      "üìÖ Date range: 2021-01-01 00:00:00 to 2021-02-28 23:55:00\n",
      "üíæ Memory usage: 1.6 MB\n",
      "\n",
      "üìã Sample Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close time</th>\n",
       "      <th>Quote asset volume</th>\n",
       "      <th>Number of trades</th>\n",
       "      <th>Taker buy base asset volume</th>\n",
       "      <th>Taker buy quote asset volume</th>\n",
       "      <th>Ignore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>28923.63</td>\n",
       "      <td>29017.50</td>\n",
       "      <td>28913.12</td>\n",
       "      <td>28975.65</td>\n",
       "      <td>182.889878</td>\n",
       "      <td>2021-01-01 00:04:59.999</td>\n",
       "      <td>5.300787e+06</td>\n",
       "      <td>5614</td>\n",
       "      <td>80.029129</td>\n",
       "      <td>2.319247e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-01 00:05:00</td>\n",
       "      <td>28975.65</td>\n",
       "      <td>28979.53</td>\n",
       "      <td>28846.28</td>\n",
       "      <td>28858.94</td>\n",
       "      <td>214.568104</td>\n",
       "      <td>2021-01-01 00:09:59.999</td>\n",
       "      <td>6.201532e+06</td>\n",
       "      <td>4928</td>\n",
       "      <td>113.761331</td>\n",
       "      <td>3.287213e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01 00:10:00</td>\n",
       "      <td>28858.94</td>\n",
       "      <td>28883.20</td>\n",
       "      <td>28690.17</td>\n",
       "      <td>28752.80</td>\n",
       "      <td>442.619587</td>\n",
       "      <td>2021-01-01 00:14:59.999</td>\n",
       "      <td>1.273741e+07</td>\n",
       "      <td>8776</td>\n",
       "      <td>199.913764</td>\n",
       "      <td>5.753595e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01 00:15:00</td>\n",
       "      <td>28752.80</td>\n",
       "      <td>28852.48</td>\n",
       "      <td>28720.91</td>\n",
       "      <td>28820.72</td>\n",
       "      <td>174.839779</td>\n",
       "      <td>2021-01-01 00:19:59.999</td>\n",
       "      <td>5.034871e+06</td>\n",
       "      <td>5208</td>\n",
       "      <td>91.542596</td>\n",
       "      <td>2.635936e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-01 00:20:00</td>\n",
       "      <td>28822.17</td>\n",
       "      <td>28846.46</td>\n",
       "      <td>28744.09</td>\n",
       "      <td>28846.46</td>\n",
       "      <td>161.316784</td>\n",
       "      <td>2021-01-01 00:24:59.999</td>\n",
       "      <td>4.645146e+06</td>\n",
       "      <td>4630</td>\n",
       "      <td>78.111722</td>\n",
       "      <td>2.249406e+06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open time      Open      High       Low     Close      Volume  \\\n",
       "0 2021-01-01 00:00:00  28923.63  29017.50  28913.12  28975.65  182.889878   \n",
       "1 2021-01-01 00:05:00  28975.65  28979.53  28846.28  28858.94  214.568104   \n",
       "2 2021-01-01 00:10:00  28858.94  28883.20  28690.17  28752.80  442.619587   \n",
       "3 2021-01-01 00:15:00  28752.80  28852.48  28720.91  28820.72  174.839779   \n",
       "4 2021-01-01 00:20:00  28822.17  28846.46  28744.09  28846.46  161.316784   \n",
       "\n",
       "               Close time  Quote asset volume  Number of trades  \\\n",
       "0 2021-01-01 00:04:59.999        5.300787e+06              5614   \n",
       "1 2021-01-01 00:09:59.999        6.201532e+06              4928   \n",
       "2 2021-01-01 00:14:59.999        1.273741e+07              8776   \n",
       "3 2021-01-01 00:19:59.999        5.034871e+06              5208   \n",
       "4 2021-01-01 00:24:59.999        4.645146e+06              4630   \n",
       "\n",
       "   Taker buy base asset volume  Taker buy quote asset volume  Ignore  \n",
       "0                    80.029129                  2.319247e+06       0  \n",
       "1                   113.761331                  3.287213e+06       0  \n",
       "2                   199.913764                  5.753595e+06       0  \n",
       "3                    91.542596                  2.635936e+06       0  \n",
       "4                    78.111722                  2.249406e+06       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Bitcoin data using common download function\n",
    "btc_data = download_crypto_data(\n",
    "    symbol=SYMBOL,\n",
    "    interval=INTERVAL, \n",
    "    data_from=DATA_FROM,\n",
    "    data_to=DATA_TO,\n",
    "    max_rows=50000,  # Limit for memory efficiency\n",
    "    # Uses Binance Vision only\n",
    ")\n",
    "\n",
    "if btc_data is not None and not btc_data.empty:\n",
    "    print(f\"\\nüìã Sample Data:\")\n",
    "    display(btc_data.head())\n",
    "else:\n",
    "    print(\"‚ùå No data available for processing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This section handles feature engineering, scaling, and data preparation for LSTM training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_features(df, lag_period=5):\n",
    "    \"\"\"\n",
    "    Create time series features for each candle with proper 5-minute interval encoding.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with OHLCV data\n",
    "        lag_period (int): Number of previous candles to use for lag features\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with time series features\n",
    "    \"\"\"\n",
    "    print(\"üîß Creating time series features...\")\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Time-based features for 5-minute intervals\n",
    "    # 5-minute intervals = 288 intervals per day (24 * 60 / 5)\n",
    "    print(\"‚è∞ Adding time-based features...\")\n",
    "    data['Hour'] = data['Open time'].dt.hour\n",
    "    data['Minute'] = data['Open time'].dt.minute\n",
    "    \n",
    "    # Calculate the 5-minute interval number within the day (0-287)\n",
    "    data['Interval_5min'] = data['Hour'] * 12 + data['Minute'] // 5\n",
    "    \n",
    "    # Cyclical encoding for 5-minute intervals\n",
    "    data['Interval_sin'] = np.sin(2 * np.pi * data['Interval_5min'] / 288)\n",
    "    data['Interval_cos'] = np.cos(2 * np.pi * data['Interval_5min'] / 288)\n",
    "    \n",
    "    # Also keep hour-based features for broader patterns\n",
    "    data['Hour_sin'] = np.sin(2 * np.pi * data['Hour'] / 24)\n",
    "    data['Hour_cos'] = np.cos(2 * np.pi * data['Hour'] / 24)\n",
    "    \n",
    "    # Price-based features\n",
    "    print(\"üí∞ Adding price-based features...\")\n",
    "    data['Price_Range'] = data['High'] - data['Low']\n",
    "    data['Price_Change'] = data['Close'] - data['Open']\n",
    "    data['Price_Change_Pct'] = (data['Close'] - data['Open']) / data['Open']\n",
    "    \n",
    "    # Volume features\n",
    "    data['Volume_MA_5'] = data['Volume'].rolling(window=5).mean()\n",
    "    data['Volume_MA_10'] = data['Volume'].rolling(window=10).mean()\n",
    "    \n",
    "    # Lag features for OHLCV\n",
    "    print(f\"üìä Creating lag features (lag period: {lag_period})...\")\n",
    "    ohlcv_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    for col in ohlcv_cols:\n",
    "        for lag in range(1, lag_period + 1):\n",
    "            data[f'{col}_Lag_{lag}'] = data[col].shift(lag)\n",
    "    \n",
    "    print(f\"‚úÖ Time series features created! Shape: {data.shape}\")\n",
    "    \n",
    "    # Show time feature examples\n",
    "    print(f\"\\n‚è∞ Time Feature Examples:\")\n",
    "    print(f\"   Hour range: {data['Hour'].min()}-{data['Hour'].max()}\")\n",
    "    print(f\"   5-min interval range: {data['Interval_5min'].min()}-{data['Interval_5min'].max()}\")\n",
    "    print(f\"   Interval_sin range: {data['Interval_sin'].min():.3f} to {data['Interval_sin'].max():.3f}\")\n",
    "    print(f\"   Interval_cos range: {data['Interval_cos'].min():.3f} to {data['Interval_cos'].max():.3f}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_sliding_windows(data, sequence_length=10, target_cols=['Open', 'High', 'Low', 'Close', 'Volume']):\n",
    "    \"\"\"\n",
    "    Create sliding windows for time series prediction.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with features\n",
    "        sequence_length (int): Number of previous candles to use for prediction\n",
    "        target_cols (list): Columns to predict\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X, y, feature_cols) - Features and targets for LSTM training\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Creating sliding windows (sequence length: {sequence_length})...\")\n",
    "    \n",
    "    # Select feature columns (exclude time and target columns)\n",
    "    feature_cols = [col for col in data.columns if col not in ['Open time', 'Close time'] + target_cols]\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    data_clean = data.dropna()\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(sequence_length, len(data_clean)):\n",
    "        # Input: sequence_length previous candles\n",
    "        X.append(data_clean[feature_cols].iloc[i-sequence_length:i].values)\n",
    "        \n",
    "        # Target: next candle's OHLCV values\n",
    "        y.append(data_clean[target_cols].iloc[i].values)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"‚úÖ Sliding windows created!\")\n",
    "    print(f\"üìä X shape: {X.shape} (samples, sequence_length, features)\")\n",
    "    print(f\"üìä y shape: {y.shape} (samples, targets)\")\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "\n",
    "def preprocess_data_for_lstm(df, test_size=TEST_SIZE):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for LSTM training.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with OHLCV data\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test, feature_cols)\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting LSTM prediction preprocessing...\")\n",
    "    \n",
    "    # Step 1: Create time series features\n",
    "    data_with_features = create_time_series_features(df, LAG_PERIOD)\n",
    "    \n",
    "    # Step 2: Create sliding windows\n",
    "    X, y, feature_cols = create_sliding_windows(data_with_features, SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Step 3: Split data\n",
    "    split_index = int(len(X) * (1 - test_size))\n",
    "    \n",
    "    X_train = X[:split_index]\n",
    "    X_test = X[split_index:]\n",
    "    y_train = y[:split_index]\n",
    "    y_test = y[split_index:]\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è Data split completed!\")\n",
    "    print(f\"üìä Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"üìä Test samples: {X_test.shape[0]}\")\n",
    "    print(f\"üìä Sequence length: {X_train.shape[1]}\")\n",
    "    print(f\"üìä Features per timestep: {X_train.shape[2]}\")\n",
    "    print(f\"üìä Target variables: {y_train.shape[1]}\")\n",
    "    \n",
    "    # Show data structure\n",
    "    print(f\"\\nüìä DATA STRUCTURE DEMONSTRATION:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"\\nüîç Sample 1:\")\n",
    "    print(f\"   Input: {X_train.shape[1]} previous candles ‚Üí Predict next candle\")\n",
    "    print(f\"   Features per candle: {X_train.shape[2]}\")\n",
    "    print(f\"   Target: {y_train.shape[1]} OHLCV values\")\n",
    "    \n",
    "    # Show the last candle's features\n",
    "    last_candle_features = X_train[0, -1, :]  # Last timestep of first sample\n",
    "    print(f\"\\n   Last candle features (first 10):\")\n",
    "    for j in range(min(10, len(feature_cols))):\n",
    "        print(f\"     {feature_cols[j]}: {last_candle_features[j]:.4f}\")\n",
    "    if len(feature_cols) > 10:\n",
    "        print(f\"     ... and {len(feature_cols) - 10} more features\")\n",
    "    \n",
    "    # Show target\n",
    "    print(f\"\\n   Target (next candle OHLCV):\")\n",
    "    target_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    for j, target in enumerate(target_cols):\n",
    "        print(f\"     {target}: {y_train[0, j]:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìà This means:\")\n",
    "    print(f\"   - Each training sample uses {X_train.shape[1]} consecutive candles\")\n",
    "    print(f\"   - Each candle has {X_train.shape[2]} features\")\n",
    "    print(f\"   - Model predicts the next candle's {y_train.shape[1]} values\")\n",
    "    print(f\"   - Total training samples: {X_train.shape[0]}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_time_series_data(X_train, X_test, y_train, y_test, feature_cols):\n",
    "    \"\"\"\n",
    "    Scale the time series data for LSTM training.\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_test: Training and test features (samples, timesteps, features)\n",
    "        y_train, y_test: Training and test targets\n",
    "        feature_cols: List of feature column names\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_X, scaler_y)\n",
    "    \"\"\"\n",
    "    print(\"üî¢ Scaling time series data...\")\n",
    "    \n",
    "    # Reshape for scaling (samples * timesteps, features)\n",
    "    X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "    X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "    \n",
    "    # Scale features\n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train_reshaped)\n",
    "    X_test_scaled = scaler_X.transform(X_test_reshaped)\n",
    "    \n",
    "    # Reshape back to (samples, timesteps, features)\n",
    "    X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
    "    X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
    "    \n",
    "    # Scale targets\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "    \n",
    "    print(\"‚úÖ Scaling completed!\")\n",
    "    print(f\"üìä Scaled X_train shape: {X_train_scaled.shape}\")\n",
    "    print(f\"üìä Scaled y_train shape: {y_train_scaled.shape}\")\n",
    "    \n",
    "    # Show scaled data examples\n",
    "    print(f\"\\nüìä SCALED DATA EXAMPLES:\")\n",
    "    print(f\"   First sample, last candle features (first 10):\")\n",
    "    for j in range(min(10, len(feature_cols))):\n",
    "        print(f\"     {feature_cols[j]}: {X_train_scaled[0, -1, j]:.4f}\")\n",
    "    \n",
    "    print(f\"   First sample target (OHLCV):\")\n",
    "    target_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    for j, target in enumerate(target_cols):\n",
    "        print(f\"     {target}: {y_train_scaled[0, j]:.4f}\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_X, scaler_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting LSTM prediction preprocessing...\n",
      "üîß Creating time series features...\n",
      "‚è∞ Adding time-based features...\n",
      "üí∞ Adding price-based features...\n",
      "üìä Creating lag features (lag period: 5)...\n",
      "‚úÖ Time series features created! Shape: (16977, 49)\n",
      "\n",
      "‚è∞ Time Feature Examples:\n",
      "   Hour range: 0-23\n",
      "   5-min interval range: 0-287\n",
      "   Interval_sin range: -1.000 to 1.000\n",
      "   Interval_cos range: -1.000 to 1.000\n",
      "üîÑ Creating sliding windows (sequence length: 10)...\n",
      "‚úÖ Sliding windows created!\n",
      "üìä X shape: (16958, 10, 42) (samples, sequence_length, features)\n",
      "üìä y shape: (16958, 5) (samples, targets)\n",
      "‚úÇÔ∏è Data split completed!\n",
      "üìä Training samples: 13566\n",
      "üìä Test samples: 3392\n",
      "üìä Sequence length: 10\n",
      "üìä Features per timestep: 42\n",
      "üìä Target variables: 5\n",
      "\n",
      "üìä DATA STRUCTURE DEMONSTRATION:\n",
      "==================================================\n",
      "\n",
      "üîç Sample 1:\n",
      "   Input: 10 previous candles ‚Üí Predict next candle\n",
      "   Features per candle: 42\n",
      "   Target: 5 OHLCV values\n",
      "\n",
      "   Last candle features (first 10):\n",
      "     Quote asset volume: 14853888.4565\n",
      "     Number of trades: 9641.0000\n",
      "     Taker buy base asset volume: 277.2814\n",
      "     Taker buy quote asset volume: 8152632.9124\n",
      "     Ignore: 0.0000\n",
      "     Hour: 1.0000\n",
      "     Minute: 30.0000\n",
      "     Interval_5min: 18.0000\n",
      "     Interval_sin: 0.3827\n",
      "     Interval_cos: 0.9239\n",
      "     ... and 32 more features\n",
      "\n",
      "   Target (next candle OHLCV):\n",
      "     Open: 29361.7700\n",
      "     High: 29470.0000\n",
      "     Low: 29340.0000\n",
      "     Close: 29400.0100\n",
      "     Volume: 494.4106\n",
      "\n",
      "üìà This means:\n",
      "   - Each training sample uses 10 consecutive candles\n",
      "   - Each candle has 42 features\n",
      "   - Model predicts the next candle's 5 values\n",
      "   - Total training samples: 13566\n",
      "‚úÖ Data preprocessing completed successfully!\n",
      "\n",
      "üìä PROCESSED DATA TABLES:\n",
      "==================================================\n",
      "\n",
      "üìã Training Features (First 10 samples, last candle):\n",
      "   Shape: (13566, 10, 42)\n",
      "   First 10 samples, last candle features (first 10):\n",
      "     Sample 1: [1.48538885e+07 9.64100000e+03 2.77281426e+02 8.15263291e+06\n",
      " 0.00000000e+00 1.00000000e+00 3.00000000e+01 1.80000000e+01\n",
      " 3.82683432e-01 9.23879533e-01]\n",
      "     Sample 2: [1.45480534e+07 8.71600000e+03 3.13759025e+02 9.23376460e+06\n",
      " 0.00000000e+00 1.00000000e+00 3.50000000e+01 1.90000000e+01\n",
      " 4.02746690e-01 9.15311479e-01]\n",
      "     Sample 3: [9.80803062e+06 6.91600000e+03 1.28887871e+02 3.78505368e+06\n",
      " 0.00000000e+00 1.00000000e+00 4.00000000e+01 2.00000000e+01\n",
      " 4.22618262e-01 9.06307787e-01]\n",
      "     Sample 4: [6.99113384e+06 5.50700000e+03 1.48761494e+02 4.36624246e+06\n",
      " 0.00000000e+00 1.00000000e+00 4.50000000e+01 2.10000000e+01\n",
      " 4.42288690e-01 8.96872742e-01]\n",
      "     Sample 5: [5.73524041e+06 6.02000000e+03 1.00480012e+02 2.95052394e+06\n",
      " 0.00000000e+00 1.00000000e+00 5.00000000e+01 2.20000000e+01\n",
      " 4.61748613e-01 8.87010833e-01]\n",
      "     Sample 6: [5.48891913e+06 4.81700000e+03 1.03596697e+02 3.04556350e+06\n",
      " 0.00000000e+00 1.00000000e+00 5.50000000e+01 2.30000000e+01\n",
      " 4.80988769e-01 8.76726756e-01]\n",
      "     Sample 7: [9.13104196e+06 5.78600000e+03 1.50624162e+02 4.43120297e+06\n",
      " 0.00000000e+00 2.00000000e+00 0.00000000e+00 2.40000000e+01\n",
      " 5.00000000e-01 8.66025404e-01]\n",
      "     Sample 8: [4.85299726e+06 3.83800000e+03 7.94999910e+01 2.33409832e+06\n",
      " 0.00000000e+00 2.00000000e+00 5.00000000e+00 2.50000000e+01\n",
      " 5.18773258e-01 8.54911871e-01]\n",
      "     Sample 9: [7.79154060e+06 6.57600000e+03 1.20158839e+02 3.51953245e+06\n",
      " 0.00000000e+00 2.00000000e+00 1.00000000e+01 2.60000000e+01\n",
      " 5.37299608e-01 8.43391446e-01]\n",
      "     Sample 10: [5.51720301e+06 4.60200000e+03 1.22860833e+02 3.59944985e+06\n",
      " 0.00000000e+00 2.00000000e+00 1.50000000e+01 2.70000000e+01\n",
      " 5.55570233e-01 8.31469612e-01]\n",
      "\n",
      "üìã Test Features (First 10 samples, last candle):\n",
      "   Shape: (3392, 10, 42)\n",
      "   First 10 samples, last candle features (first 10):\n",
      "     Sample 1: [7.81422799e+06 4.33500000e+03 7.20578500e+01 3.55871960e+06\n",
      " 0.00000000e+00 5.00000000e+00 1.50000000e+01 6.30000000e+01\n",
      " 9.80785280e-01 1.95090322e-01]\n",
      "     Sample 2: [9.11203311e+06 4.93800000e+03 6.81306210e+01 3.36276962e+06\n",
      " 0.00000000e+00 5.00000000e+00 2.00000000e+01 6.40000000e+01\n",
      " 9.84807753e-01 1.73648178e-01]\n",
      "     Sample 3: [1.35394151e+07 5.12300000e+03 9.68425360e+01 4.77622154e+06\n",
      " 0.00000000e+00 5.00000000e+00 2.50000000e+01 6.50000000e+01\n",
      " 9.88361510e-01 1.52123386e-01]\n",
      "     Sample 4: [1.18587800e+07 7.21500000e+03 1.26224500e+02 6.25048558e+06\n",
      " 0.00000000e+00 5.00000000e+00 3.00000000e+01 6.60000000e+01\n",
      " 9.91444861e-01 1.30526192e-01]\n",
      "     Sample 5: [1.12953933e+07 7.00400000e+03 9.99819450e+01 4.95311434e+06\n",
      " 0.00000000e+00 5.00000000e+00 3.50000000e+01 6.70000000e+01\n",
      " 9.94056338e-01 1.08866875e-01]\n",
      "     Sample 6: [8.69407860e+06 5.05900000e+03 8.60023350e+01 4.26232433e+06\n",
      " 0.00000000e+00 5.00000000e+00 4.00000000e+01 6.80000000e+01\n",
      " 9.96194698e-01 8.71557427e-02]\n",
      "     Sample 7: [7.44097978e+06 4.44000000e+03 6.24521010e+01 3.09509056e+06\n",
      " 0.00000000e+00 5.00000000e+00 4.50000000e+01 6.90000000e+01\n",
      " 9.97858923e-01 6.54031292e-02]\n",
      "     Sample 8: [6.31207415e+06 4.55100000e+03 5.84386680e+01 2.89724645e+06\n",
      " 0.00000000e+00 5.00000000e+00 5.00000000e+01 7.00000000e+01\n",
      " 9.99048222e-01 4.36193874e-02]\n",
      "     Sample 9: [9.81134221e+06 5.75800000e+03 9.84877330e+01 4.89057064e+06\n",
      " 0.00000000e+00 5.00000000e+00 5.50000000e+01 7.10000000e+01\n",
      " 9.99762027e-01 2.18148850e-02]\n",
      "     Sample 10: [1.17223007e+07 6.96500000e+03 1.21175663e+02 6.02694999e+06\n",
      " 0.00000000e+00 6.00000000e+00 0.00000000e+00 7.20000000e+01\n",
      " 1.00000000e+00 6.12323400e-17]\n",
      "\n",
      "üìã Training Targets (First 10 samples):\n",
      "   Shape: (13566, 5)\n",
      "   First 10 samples targets (OHLCV):\n",
      "     Sample 1: [29361.77     29470.       29340.       29400.01       494.410646]\n",
      "     Sample 2: [29401.66     29415.8      29312.17     29319.87       334.005053]\n",
      "     Sample 3: [29320.7      29394.23     29311.75     29389.47       238.197783]\n",
      "     Sample 4: [29389.48     29394.46     29338.96     29371.22       195.309364]\n",
      "     Sample 5: [29371.23     29430.85     29352.26     29409.99       186.720557]\n",
      "     Sample 6: [29410.       29465.26     29359.99     29382.73       310.385444]\n",
      "     Sample 7: [29382.73     29386.95     29335.       29375.07       165.302054]\n",
      "     Sample 8: [29375.08     29377.85     29201.78     29248.69       266.004274]\n",
      "     Sample 9: [29248.69     29333.       29240.41     29333.         188.312749]\n",
      "     Sample 10: [29333.       29388.22     29305.01     29362.3        139.327516]\n",
      "\n",
      "üìã Test Targets (First 10 samples):\n",
      "   Shape: (3392, 5)\n",
      "   First 10 samples targets (OHLCV):\n",
      "     Sample 1: [49396.95     49438.76     49300.       49311.78       184.627233]\n",
      "     Sample 2: [49311.79     49381.48     49227.68     49378.17       274.603176]\n",
      "     Sample 3: [49378.17     49596.15     49369.66     49519.42       239.460257]\n",
      "     Sample 4: [49519.43     49668.76     49414.88     49531.25       228.016791]\n",
      "     Sample 5: [49531.24     49651.24     49472.56     49533.61       175.407328]\n",
      "     Sample 6: [49533.62     49618.15     49505.01     49512.44       150.147518]\n",
      "     Sample 7: [49512.45     49629.23     49512.44     49603.24       127.317525]\n",
      "     Sample 8: [49603.25     49731.03     49580.       49706.8        197.602013]\n",
      "     Sample 9: [49706.8      49787.24     49634.71     49635.95       235.696303]\n",
      "     Sample 10: [49638.87     49824.85     49624.06     49820.         195.822761]\n",
      "\n",
      "üìä Data Shapes Summary:\n",
      "Training features: (13566, 10, 42) (samples, sequence_length, features)\n",
      "Test features: (3392, 10, 42) (samples, sequence_length, features)\n",
      "Training targets: (13566, 5) (samples, targets)\n",
      "Test targets: (3392, 5) (samples, targets)\n",
      "\n",
      "üìä Feature Information:\n",
      "   Total features: 42\n",
      "   Feature names (first 10): ['Quote asset volume', 'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume', 'Ignore', 'Hour', 'Minute', 'Interval_5min', 'Interval_sin', 'Interval_cos']\n",
      "   Target variables: ['Open', 'High', 'Low', 'Close', 'Volume']\n",
      "\n",
      "üìä Data Range Information:\n",
      "   X_train min: -1297.4300, max: 174859111.1828\n",
      "   y_train min: 0.0000, max: 50689.1800\n",
      "   X_test min: -1344.7400, max: 226332647.4954\n",
      "   y_test min: 53.8240, max: 58352.8000\n"
     ]
    }
   ],
   "source": [
    "if btc_data is not None:\n",
    "    X_train, X_test, y_train, y_test, feature_cols = preprocess_data_for_lstm(btc_data)\n",
    "    print(\"‚úÖ Data preprocessing completed successfully!\")\n",
    "    \n",
    "    # Show processed data tables\n",
    "    print(\"\\nüìä PROCESSED DATA TABLES:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nüìã Training Features (First 10 samples, last candle):\")\n",
    "    print(\"   Shape:\", X_train.shape)\n",
    "    print(\"   First 10 samples, last candle features (first 10):\")\n",
    "    for i in range(min(10, len(X_train))):\n",
    "        print(f\"     Sample {i+1}: {X_train[i, -1, :10]}\")\n",
    "    \n",
    "    print(\"\\nüìã Test Features (First 10 samples, last candle):\")\n",
    "    print(\"   Shape:\", X_test.shape)\n",
    "    print(\"   First 10 samples, last candle features (first 10):\")\n",
    "    for i in range(min(10, len(X_test))):\n",
    "        print(f\"     Sample {i+1}: {X_test[i, -1, :10]}\")\n",
    "    \n",
    "    print(\"\\nüìã Training Targets (First 10 samples):\")\n",
    "    print(\"   Shape:\", y_train.shape)\n",
    "    print(\"   First 10 samples targets (OHLCV):\")\n",
    "    for i in range(min(10, len(y_train))):\n",
    "        print(f\"     Sample {i+1}: {y_train[i]}\")\n",
    "    \n",
    "    print(\"\\nüìã Test Targets (First 10 samples):\")\n",
    "    print(\"   Shape:\", y_test.shape)\n",
    "    print(\"   First 10 samples targets (OHLCV):\")\n",
    "    for i in range(min(10, len(y_test))):\n",
    "        print(f\"     Sample {i+1}: {y_test[i]}\")\n",
    "    \n",
    "    print(\"\\nüìä Data Shapes Summary:\")\n",
    "    print(f\"Training features: {X_train.shape} (samples, sequence_length, features)\")\n",
    "    print(f\"Test features: {X_test.shape} (samples, sequence_length, features)\")\n",
    "    print(f\"Training targets: {y_train.shape} (samples, targets)\")\n",
    "    print(f\"Test targets: {y_test.shape} (samples, targets)\")\n",
    "    \n",
    "    print(f\"\\nüìä Feature Information:\")\n",
    "    print(f\"   Total features: {len(feature_cols)}\")\n",
    "    print(f\"   Feature names (first 10): {feature_cols[:10]}\")\n",
    "    print(f\"   Target variables: ['Open', 'High', 'Low', 'Close', 'Volume']\")\n",
    "    \n",
    "    print(f\"\\nüìä Data Range Information:\")\n",
    "    print(f\"   X_train min: {X_train.min():.4f}, max: {X_train.max():.4f}\")\n",
    "    print(f\"   y_train min: {y_train.min():.4f}, max: {y_train.max():.4f}\")\n",
    "    print(f\"   X_test min: {X_test.min():.4f}, max: {X_test.max():.4f}\")\n",
    "    print(f\"   y_test min: {y_test.min():.4f}, max: {y_test.max():.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data for LSTM training\n",
    "if 'X_train' in locals() and 'X_test' in locals() and 'y_train' in locals() and 'y_test' in locals():\n",
    "    X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_X, scaler_y = scale_data(X_train, X_test, y_train, y_test)\n",
    "    print(\"‚úÖ Data scaling completed successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå No data available for scaling. Please run preprocessing first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "This section defines the LSTM model architecture for multi-output time series prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape, output_shape, units=50, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Build and compile an LSTM model for multi-output time series prediction.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): The shape of the input data (timesteps, num_features)\n",
    "        output_shape (int): The number of output features\n",
    "        units (int): The number of units in the LSTM layer\n",
    "        dropout_rate (float): The dropout rate for regularization\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: The compiled LSTM model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(units=units, activation='relu', input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(units=output_shape)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model(input_shape, output_shape):\n",
    "    \"\"\"\n",
    "    Create and return a configured LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Input shape for the model\n",
    "        output_shape (int): Number of output features\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled LSTM model\n",
    "    \"\"\"\n",
    "    model = build_lstm_model(input_shape, output_shape)\n",
    "    \n",
    "    print(\"LSTM Model Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bitcoin data using common download function\n",
    "btc_data = download_crypto_data(\n",
    "    symbol=SYMBOL,\n",
    "    interval=INTERVAL, \n",
    "    data_from=DATA_FROM,\n",
    "    data_to=DATA_TO,\n",
    "    max_rows=50000,  # Limit for memory efficiency\n",
    "    # Uses Binance Vision only\n",
    ")\n",
    "\n",
    "if btc_data is not None and not btc_data.empty:\n",
    "    print(f\"\\\\nüìã Sample Data:\")\n",
    "    display(btc_data.head())\n",
    "else:\n",
    "    print(\"‚ùå No data available for processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    output_shape = y_train.shape[1]\n",
    "    \n",
    "    model = create_model(input_shape, output_shape)\n",
    "    print(\"Model created successfully!\")\n",
    "else:\n",
    "    print(\"Training data not available. Please run preprocessing first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "This section handles model training, evaluation, and visualization of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train the LSTM model and return training history.\n",
    "    \n",
    "    Args:\n",
    "        model: The LSTM model to train\n",
    "        X_train: Training features\n",
    "        y_train: Training targets\n",
    "        X_test: Test features\n",
    "        y_test: Test targets\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.callbacks.History: Training history\n",
    "    \"\"\"\n",
    "    print(\"Starting model training...\")\n",
    "    \n",
    "    # Clean data to remove any infinite values\n",
    "    train_mask = np.all(np.isfinite(X_train), axis=(1, 2)) & np.all(np.isfinite(y_train), axis=1)\n",
    "    test_mask = np.all(np.isfinite(X_test), axis=(1, 2)) & np.all(np.isfinite(y_test), axis=1)\n",
    "    \n",
    "    X_train_clean = X_train[train_mask]\n",
    "    y_train_clean = y_train[train_mask]\n",
    "    X_test_clean = X_test[test_mask]\n",
    "    y_test_clean = y_test[test_mask]\n",
    "    \n",
    "    print(f\"Training on {len(X_train_clean)} samples\")\n",
    "    print(f\"Testing on {len(X_test_clean)} samples\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_clean, y_train_clean,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss = model.evaluate(X_test_clean, y_test_clean, verbose=0)\n",
    "    print(f\"Test Loss: {test_loss[0]:.6f}\")\n",
    "    print(f\"Test MAE: {test_loss[1]:.6f}\")\n",
    "    \n",
    "    return history, X_test_clean, y_test_clean\n",
    "\n",
    "\n",
    "def evaluate_model_performance(model, X_test, y_test, scaler_y, y_test_orig):\n",
    "    \"\"\"\n",
    "    Evaluate model performance and create visualizations.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        X_test: Test features\n",
    "        y_test: Test targets (scaled)\n",
    "        scaler_y: Target scaler for inverse transformation\n",
    "        y_test_orig: Original test targets (unscaled)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Performance metrics\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    predictions_scaled = model.predict(X_test)\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "    y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "    \n",
    "    # Calculate MSE for each target\n",
    "    target_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    mse_scores = {}\n",
    "    \n",
    "    print(\"Model Performance (MSE):\")\n",
    "    for i, col in enumerate(target_columns):\n",
    "        mse = mean_squared_error(y_test_actual[:, i], predictions[:, i])\n",
    "        mse_scores[col] = mse\n",
    "        print(f\"{col}: {mse:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'actual': y_test_actual,\n",
    "        'mse_scores': mse_scores,\n",
    "        'target_columns': target_columns\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss.\n",
    "    \n",
    "    Args:\n",
    "        history: Training history from model.fit()\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions_vs_actual(predictions, actual, target_columns, max_samples=1000):\n",
    "    \"\"\"\n",
    "    Plot predictions vs actual values for each target.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted values\n",
    "        actual: Actual values\n",
    "        target_columns: List of target column names\n",
    "        max_samples: Maximum number of samples to plot\n",
    "    \"\"\"\n",
    "    n_samples = min(len(predictions), max_samples)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(target_columns), 1, figsize=(15, 3 * len(target_columns)))\n",
    "    if len(target_columns) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(target_columns):\n",
    "        axes[i].plot(actual[:n_samples, i], label=f'Actual {col}', alpha=0.7)\n",
    "        axes[i].plot(predictions[:n_samples, i], label=f'Predicted {col}', alpha=0.7)\n",
    "        axes[i].set_title(f'{col} - Actual vs Predicted')\n",
    "        axes[i].set_xlabel('Time Steps')\n",
    "        axes[i].set_ylabel(col)\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in locals() and 'X_train' in locals() and 'y_train' in locals():\n",
    "    history, X_test_clean, y_test_clean = train_model(model, X_train, y_train, X_test, y_test, epochs=50)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    results = evaluate_model_performance(model, X_test_clean, y_test_clean, scaler_y, y_test_orig)\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    plot_predictions_vs_actual(\n",
    "        results['predictions'], \n",
    "        results['actual'], \n",
    "        results['target_columns']\n",
    "    )\n",
    "    \n",
    "    print(\"Training and evaluation completed!\")\n",
    "else:\n",
    "    print(\"Model or training data not available. Please run previous cells first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Functions\n",
    "\n",
    "This section provides functions to make predictions on new data using the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_step(model, new_data_df, scaler_X, scaler_y, timesteps=1):\n",
    "    \"\"\"\n",
    "    Make predictions on new data using the trained LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        new_data_df (pd.DataFrame): DataFrame containing historical data for feature calculation\n",
    "        scaler_X: Fitted scaler for input features\n",
    "        scaler_y: Fitted scaler for target variables\n",
    "        timesteps (int): Number of timesteps the LSTM expects\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Predictions for the next time step\n",
    "    \"\"\"\n",
    "    if new_data_df is None or new_data_df.empty:\n",
    "        print(\"Error: No new data provided for prediction.\")\n",
    "        return None\n",
    "    \n",
    "    # Create features using the same preprocessing as training\n",
    "    features_df = create_features(new_data_df)\n",
    "    \n",
    "    # Remove original features (keep only engineered ones)\n",
    "    features_df = features_df.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume', 'Hour_sin', 'Hour_cos'])\n",
    "    \n",
    "    # Get the last valid row of features\n",
    "    latest_features = features_df.dropna().iloc[[-1]].copy()\n",
    "    \n",
    "    if latest_features.empty:\n",
    "        print(\"Error: Not enough valid data to calculate features for prediction.\")\n",
    "        return None\n",
    "    \n",
    "    # Scale the features\n",
    "    latest_features_scaled = scaler_X.transform(latest_features)\n",
    "    \n",
    "    # Reshape for LSTM input\n",
    "    latest_features_lstm = latest_features_scaled.reshape((1, timesteps, latest_features_scaled.shape[1]))\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions_scaled = model.predict(latest_features_lstm)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    target_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    prediction_df = pd.DataFrame(predictions, columns=target_columns)\n",
    "    \n",
    "    return prediction_df\n",
    "\n",
    "\n",
    "def generate_sample_data(original_data, n_samples=100, noise_factor=0.001):\n",
    "    \"\"\"\n",
    "    Generate sample data for testing predictions.\n",
    "    \n",
    "    Args:\n",
    "        original_data: Original DataFrame to use as template\n",
    "        n_samples: Number of samples to generate\n",
    "        noise_factor: Amount of noise to add (as fraction of standard deviation)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Generated sample data\n",
    "    \"\"\"\n",
    "    # Take the last n_samples from original data\n",
    "    sample_data = original_data.tail(n_samples).copy()\n",
    "    \n",
    "    # Add small amount of noise to make it different\n",
    "    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "        noise = np.random.randn(len(sample_data)) * sample_data[col].std() * noise_factor\n",
    "        sample_data[col] = sample_data[col] + noise\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "\n",
    "def demonstrate_prediction(model, scaler_X, scaler_y, original_data):\n",
    "    \"\"\"\n",
    "    Demonstrate prediction functionality with sample data.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        scaler_X: Fitted scaler for input features\n",
    "        scaler_y: Fitted scaler for target variables\n",
    "        original_data: Original data to use for generating samples\n",
    "    \"\"\"\n",
    "    print(\"Generating sample data for prediction demonstration...\")\n",
    "    \n",
    "    # Generate sample data\n",
    "    sample_data = generate_sample_data(original_data, n_samples=100)\n",
    "    \n",
    "    print(f\"Sample data shape: {sample_data.shape}\")\n",
    "    print(\"Last 5 rows of sample data:\")\n",
    "    display(sample_data[['Open time', 'Open', 'High', 'Low', 'Close', 'Volume']].tail())\n",
    "    \n",
    "    # Make prediction\n",
    "    print(\"\\nMaking prediction for next time step...\")\n",
    "    prediction = predict_next_step(model, sample_data, scaler_X, scaler_y)\n",
    "    \n",
    "    if prediction is not None:\n",
    "        print(\"Predicted values for next time step:\")\n",
    "        display(prediction)\n",
    "        \n",
    "        # Show the actual next values for comparison (if available)\n",
    "        if len(original_data) > len(sample_data):\n",
    "            actual_next = original_data.iloc[len(sample_data):len(sample_data)+1][['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "            print(\"\\nActual values for comparison:\")\n",
    "            display(actual_next)\n",
    "    else:\n",
    "        print(\"Failed to generate prediction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate prediction functionality\n",
    "if 'model' in locals() and 'scaler_X' in locals() and 'scaler_y' in locals() and 'btc_data' in locals():\n",
    "    demonstrate_prediction(model, scaler_X, scaler_y, btc_data)\n",
    "else:\n",
    "    print(\"Model, scalers, or data not available. Please run training first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Visualization Functions\n",
    "\n",
    "This section provides additional visualization utilities for analyzing the data and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price_data(df, title=\"Cryptocurrency Price Data\", max_points=2000):\n",
    "    \"\"\"\n",
    "    Plot the original price data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLCV data\n",
    "        title: Plot title\n",
    "        max_points: Maximum number of points to plot\n",
    "    \"\"\"\n",
    "    n_points = min(len(df), max_points)\n",
    "    data_subset = df.tail(n_points)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Price subplot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(data_subset['Open time'], data_subset['Close'], label='Close Price', alpha=0.8)\n",
    "    plt.title('Close Price Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price (USDT)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Volume subplot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(data_subset['Open time'], data_subset['Volume'], label='Volume', color='orange', alpha=0.8)\n",
    "    plt.title('Volume Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Volume')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # OHLC subplot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(data_subset['Open time'], data_subset['Open'], label='Open', alpha=0.7)\n",
    "    plt.plot(data_subset['Open time'], data_subset['High'], label='High', alpha=0.7)\n",
    "    plt.plot(data_subset['Open time'], data_subset['Low'], label='Low', alpha=0.7)\n",
    "    plt.plot(data_subset['Open time'], data_subset['Close'], label='Close', alpha=0.7)\n",
    "    plt.title('OHLC Prices')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price (USDT)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Price distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(data_subset['Close'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Close Price Distribution')\n",
    "    plt.xlabel('Price (USDT)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_prediction_errors(predictions, actual, target_columns):\n",
    "    \"\"\"\n",
    "    Plot prediction errors for each target variable.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted values\n",
    "        actual: Actual values\n",
    "        target_columns: List of target column names\n",
    "    \"\"\"\n",
    "    errors = actual - predictions\n",
    "    \n",
    "    fig, axes = plt.subplots(len(target_columns), 1, figsize=(15, 3 * len(target_columns)))\n",
    "    if len(target_columns) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(target_columns):\n",
    "        axes[i].plot(errors[:, i], label=f'Error ({col})', alpha=0.7, color='red')\n",
    "        axes[i].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        axes[i].set_title(f'Prediction Error for {col}')\n",
    "        axes[i].set_xlabel('Time Steps')\n",
    "        axes[i].set_ylabel('Error')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_importance(feature_names, model_weights=None):\n",
    "    \"\"\"\n",
    "    Plot feature importance if available.\n",
    "    \n",
    "    Args:\n",
    "        feature_names: List of feature names\n",
    "        model_weights: Model weights (if available)\n",
    "    \"\"\"\n",
    "    if model_weights is None:\n",
    "        print(\"Feature importance not available for LSTM models.\")\n",
    "        return\n",
    "    \n",
    "    # This is a placeholder - LSTM feature importance is complex\n",
    "    # In practice, you might use permutation importance or SHAP values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(feature_names)), model_weights)\n",
    "    plt.title('Feature Importance (Placeholder)')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_summary_report(results, model_history=None):\n",
    "    \"\"\"\n",
    "    Create a summary report of model performance.\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from evaluate_model_performance\n",
    "        model_history: Training history (optional)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nMSE Scores:\")\n",
    "    for col, mse in results['mse_scores'].items():\n",
    "        print(f\"  {col}: {mse:.4f}\")\n",
    "    \n",
    "    # Calculate overall performance\n",
    "    avg_mse = np.mean(list(results['mse_scores'].values()))\n",
    "    print(f\"\\nAverage MSE: {avg_mse:.4f}\")\n",
    "    \n",
    "    if model_history is not None:\n",
    "        final_train_loss = model_history.history['loss'][-1]\n",
    "        final_val_loss = model_history.history['val_loss'][-1]\n",
    "        print(f\"\\nFinal Training Loss: {final_train_loss:.4f}\")\n",
    "        print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(\"  - Type: LSTM (Long Short-Term Memory)\")\n",
    "    print(\"  - Input Shape: (timesteps, features)\")\n",
    "    print(\"  - Output: Multi-output regression\")\n",
    "    print(\"  - Targets: Open, High, Low, Close, Volume\")\n",
    "    \n",
    "    print(\"\\nData Information:\")\n",
    "    print(f\"  - Training samples: {len(results['actual'])}\")\n",
    "    print(f\"  - Features per sample: {results['predictions'].shape[1]}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional visualizations and analysis\n",
    "if 'btc_data' in locals() and btc_data is not None:\n",
    "    print(\"Plotting original price data...\")\n",
    "    plot_price_data(btc_data, \"Bitcoin Price Data (2021)\")\n",
    "    \n",
    "if 'results' in locals() and results is not None:\n",
    "    print(\"\\nPlotting prediction errors...\")\n",
    "    plot_prediction_errors(results['predictions'], results['actual'], results['target_columns'])\n",
    "    \n",
    "    print(\"\\nGenerating summary report...\")\n",
    "    if 'history' in locals():\n",
    "        create_summary_report(results, history)\n",
    "    else:\n",
    "        create_summary_report(results)\n",
    "else:\n",
    "    print(\"Data or results not available for additional visualizations.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
