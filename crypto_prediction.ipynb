{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 0: Imports and Setup\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from IPython.display import display\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Configure matplotlib for notebook environment\n",
        "plt.style.use('default')\n",
        "\n",
        "# Import our integrated utilities\n",
        "from src import (\n",
        "    BinanceDataOrganizer, DataConfig, GroupedScaler,\n",
        "    create_lstm_model, evaluate_model\n",
        ")\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"üìä TensorFlow version: {tf.__version__}\")\n",
        "print(f\"üìä NumPy version: {np.__version__}\")\n",
        "print(f\"üìä Pandas version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration and Data Setup\n",
        "\n",
        "Configure the data parameters and create the BinanceDataOrganizer instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Configuration\n",
        "# Configuration parameters\n",
        "SYMBOL = \"BTCUSDT\"\n",
        "TIMEFRAME = \"5m\"\n",
        "START_DATE = \"2021-01\"  # Updated to yyyy-mm format\n",
        "END_DATE = \"2021-01\"    # Updated to yyyy-mm format\n",
        "SEQUENCE_LENGTH = 30      # Number of timesteps to look back\n",
        "PREDICTION_LENGTH = 10    # Number of future timesteps to predict\n",
        "TRAIN_SPLIT = 0.8        # 80% for training, 20% for testing\n",
        "\n",
        "# Model parameters\n",
        "LSTM_UNITS = 100\n",
        "DROPOUT_RATE = 0.2\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Create configuration\n",
        "config = DataConfig(\n",
        "    symbol=SYMBOL,\n",
        "    timeframe=TIMEFRAME,\n",
        "    start_time=START_DATE,\n",
        "    end_time=END_DATE,\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    prediction_length=PREDICTION_LENGTH,\n",
        "    train_split=TRAIN_SPLIT\n",
        ")\n",
        "\n",
        "print(f\"üîß Configuration created:\")\n",
        "print(f\"   Symbol: {config.symbol}\")\n",
        "print(f\"   Timeframe: {config.timeframe}\")\n",
        "print(f\"   Period: {config.start_time} to {config.end_time}\")\n",
        "print(f\"   Sequence Length: {config.sequence_length}\")\n",
        "print(f\"   Prediction Length: {config.prediction_length}\")\n",
        "print(f\"   Train Split: {config.train_split}\")\n",
        "print(f\"\\nüìä Input Structure: OHLCV only\")\n",
        "print(f\"   Features: 5 (Open, High, Low, Close, Volume)\")\n",
        "print(f\"   Targets: 5 (Open, High, Low, Close, Volume)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Create and Initialize Organizer\n",
        "# Create the BinanceDataOrganizer instance\n",
        "organizer = BinanceDataOrganizer(config)\n",
        "\n",
        "print(\"üöÄ Starting complete data processing pipeline...\")\n",
        "\n",
        "# Process all data (load + create features)\n",
        "if organizer.process_all():\n",
        "    print(\"‚úÖ Data processing completed successfully!\")\n",
        "    \n",
        "    # Get feature information\n",
        "    feature_info = organizer.get_feature_info()\n",
        "    print(f\"\\nüìä Feature Information:\")\n",
        "    print(f\"   Total features: {feature_info['num_features']}\")\n",
        "    print(f\"   Sequence length: {feature_info['sequence_length']}\")\n",
        "    print(f\"   Prediction length: {feature_info['prediction_length']}\")\n",
        "    print(f\"   Data shape: {feature_info['data_shape']}\")\n",
        "    print(f\"   Total sequences: {feature_info['total_sequences']}\")\n",
        "    print(f\"\\nüîß Feature columns: {feature_info['feature_columns']}\")\n",
        "    \n",
        "    # Display data summary\n",
        "    data_summary = organizer.get_data_summary()\n",
        "    print(f\"\\nüìä Data Summary:\")\n",
        "    print(f\"   Total rows: {data_summary['total_rows']}\")\n",
        "    print(f\"   Date range: {data_summary['date_range']}\")\n",
        "    print(f\"   Columns: {data_summary['columns']}\")\n",
        "else:\n",
        "    print(\"‚ùå Data processing failed!\")\n",
        "    raise Exception(\"Failed to process data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Visualization and Analysis\n",
        "\n",
        "Visualize the loaded data and analyze its characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Data Visualization\n",
        "print(\"üìä Visualizing loaded data...\")\n",
        "\n",
        "# Get unscaled data for visualization\n",
        "unscaled_data = organizer.get_unscaled_data('all')\n",
        "\n",
        "# Display basic statistics\n",
        "print(f\"\\nüìà Data Statistics:\")\n",
        "print(f\"   X_train shape: {unscaled_data['X_train'].shape}\")\n",
        "print(f\"   y_train shape: {unscaled_data['y_train'].shape}\")\n",
        "print(f\"   X_test shape: {unscaled_data['X_test'].shape}\")\n",
        "print(f\"   y_test shape: {unscaled_data['y_test'].shape}\")\n",
        "\n",
        "# Show sample data\n",
        "print(f\"\\nüìä Sample Input Data (OHLCV):\")\n",
        "print(f\"   First sequence shape: {unscaled_data['X_train'][0].shape}\")\n",
        "print(f\"   First sequence (Open): {unscaled_data['X_train'][0][:, 0]}\")\n",
        "print(f\"   First sequence (High): {unscaled_data['X_train'][0][:, 1]}\")\n",
        "print(f\"   First sequence (Low): {unscaled_data['X_train'][0][:, 2]}\")\n",
        "print(f\"   First sequence (Close): {unscaled_data['X_train'][0][:, 3]}\")\n",
        "print(f\"   First sequence (Volume): {unscaled_data['X_train'][0][:, 4]}\")\n",
        "\n",
        "print(f\"\\nüìä Sample Target Data (OHLCV):\")\n",
        "print(f\"   First target: {unscaled_data['y_train'][0]}\")\n",
        "print(f\"   Second target: {unscaled_data['y_train'][1]}\")\n",
        "\n",
        "print(f\"\\nüìä Data Structure:\")\n",
        "print(f\"   Input features: 5 (Open, High, Low, Close, Volume)\")\n",
        "print(f\"   Target values: 5 (Open, High, Low, Close, Volume)\")\n",
        "print(f\"   Sequence length: {config.sequence_length} timesteps\")\n",
        "print(f\"   Total sequences: {len(unscaled_data['X_train']) + len(unscaled_data['X_test'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1. Simplified Data Structure Explanation\n",
        "\n",
        "The data has been simplified to use only essential OHLCV data. The model uses OHLCV values as both input features and target predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3.1: Demonstrate Simplified Data Structure\n",
        "print(\"üìä SIMPLIFIED DATA STRUCTURE DEMONSTRATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Show the essential data structure\n",
        "print(f\"\\n1Ô∏è‚É£ Essential Data Columns:\")\n",
        "print(f\"   {list(organizer.raw_data.columns)}\")\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£ Input Sequences (X):\")\n",
        "print(f\"   Shape: {unscaled_data['X_train'].shape}\")\n",
        "print(f\"   Meaning: [samples, timesteps, features]\")\n",
        "print(f\"   Features: 5 (Open, High, Low, Close, Volume)\")\n",
        "\n",
        "print(f\"\\n3Ô∏è‚É£ Target Sequences (y):\")\n",
        "print(f\"   Shape: {unscaled_data['y_train'].shape}\")\n",
        "print(f\"   Meaning: [samples, prediction_values]\")\n",
        "print(f\"   Values: 5 (Open, High, Low, Close, Volume)\")\n",
        "\n",
        "print(f\"\\n4Ô∏è‚É£ Example Sequence:\")\n",
        "print(f\"   Input (OHLCV): {unscaled_data['X_train'][0]}\")\n",
        "print(f\"   Target (OHLCV): {unscaled_data['y_train'][0]}\")\n",
        "\n",
        "print(f\"\\n5Ô∏è‚É£ Price Pattern Analysis:\")\n",
        "sample_sequence = unscaled_data['X_train'][0]\n",
        "print(f\"   Open prices: {sample_sequence[:, 0]}\")\n",
        "print(f\"   High prices: {sample_sequence[:, 1]}\")\n",
        "print(f\"   Low prices: {sample_sequence[:, 2]}\")\n",
        "print(f\"   Close prices: {sample_sequence[:, 3]}\")\n",
        "print(f\"   Volume: {sample_sequence[:, 4]}\")\n",
        "\n",
        "print(f\"\\n‚úÖ This simplified structure uses OHLCV patterns to predict future OHLCV values!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training with Integrated Normalization\n",
        "\n",
        "Train the LSTM model using the integrated normalization system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Get Scaled Data and Train Model\n",
        "# Get scaled data (this will fit scalers if not already fitted)\n",
        "print(\"üî¢ Getting scaled data with integrated normalization...\")\n",
        "scaled_data = organizer.get_scaled_data('all')\n",
        "\n",
        "X_train_scaled = scaled_data['X_train_scaled']\n",
        "X_test_scaled = scaled_data['X_test_scaled']\n",
        "y_train_scaled = scaled_data['y_train_scaled']\n",
        "y_test_scaled = scaled_data['y_test_scaled']\n",
        "\n",
        "print(f\"‚úÖ Scaled data ready:\")\n",
        "print(f\"   X_train_scaled: {X_train_scaled.shape}\")\n",
        "print(f\"   y_train_scaled: {y_train_scaled.shape}\")\n",
        "print(f\"   X_train range: {X_train_scaled.min():.4f} to {X_train_scaled.max():.4f}\")\n",
        "print(f\"   y_train range: {y_train_scaled.min():.4f} to {y_train_scaled.max():.4f}\")\n",
        "\n",
        "# Get scalers for later use\n",
        "scalers = organizer.get_scalers()\n",
        "scaler_X = scalers['X']  # GroupedScaler\n",
        "scaler_y = scalers['y']  # MinMaxScaler\n",
        "\n",
        "# Display feature groups\n",
        "feature_groups = scaler_X.get_feature_groups()\n",
        "print(f\"\\nüìä Feature Groups:\")\n",
        "for group_name, features in feature_groups.items():\n",
        "    print(f\"   {group_name.capitalize()}: {features}\")\n",
        "\n",
        "print(f\"\\nüìä Simplified Input Structure:\")\n",
        "print(f\"   Input shape: {X_train_scaled.shape}\")\n",
        "print(f\"   Features per timestep: {X_train_scaled.shape[2]} (OHLCV)\")\n",
        "print(f\"   Target shape: {y_train_scaled.shape}\")\n",
        "print(f\"   Target values: 5 (Open, High, Low, Close, Volume)\")\n",
        "print(f\"   Sequence length: {X_train_scaled.shape[1]} timesteps\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Create and Train LSTM Model\n",
        "print(f\"üèóÔ∏è Creating LSTM model...\")\n",
        "print(f\"   LSTM units: {LSTM_UNITS}\")\n",
        "print(f\"   Dropout rate: {DROPOUT_RATE}\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Input shape: {X_train_scaled.shape[1:]} (timesteps, features)\")\n",
        "print(f\"   Output shape: {y_train_scaled.shape[1]} (OHLCV values)\")\n",
        "print(f\"   Prediction length: {PREDICTION_LENGTH}\")\n",
        "\n",
        "# Create model\n",
        "model = create_lstm_model(\n",
        "    input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]),\n",
        "    lstm_units=LSTM_UNITS,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    prediction_length=PREDICTION_LENGTH\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model created with {model.count_params():,} parameters\")\n",
        "print(f\"\\nüìã Model Architecture:\")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Train the Model\n",
        "print(f\"üöÄ Starting model training...\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train_scaled,\n",
        "    validation_data=(X_test_scaled, y_test_scaled),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-7)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Plot Training History\n",
        "print(\"üìà Plotting training history...\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Training MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation and Advanced Features\n",
        "\n",
        "Evaluate the trained model and demonstrate advanced features of the BinanceDataOrganizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Model Evaluation\n",
        "print(f\"üìä Evaluating model performance...\")\n",
        "\n",
        "# Evaluate model\n",
        "evaluation_results = evaluate_model(\n",
        "    model=model,\n",
        "    X_test=X_test_scaled,\n",
        "    y_test=y_test_scaled,\n",
        "    scaler_y=scaler_y\n",
        ")\n",
        "\n",
        "print(f\"\\nüìà Evaluation Results:\")\n",
        "print(f\"   Test Loss (MSE): {evaluation_results['test_loss']:.6f}\")\n",
        "print(f\"   Test MAE: {evaluation_results['test_mae']:.6f}\")\n",
        "print(f\"   Test MAPE: {evaluation_results['test_mape']:.2f}%\")\n",
        "print(f\"   RMSE: {evaluation_results['rmse']:.6f}\")\n",
        "\n",
        "# Get predictions for visualization\n",
        "y_pred = evaluation_results['predictions']\n",
        "y_true = evaluation_results['y_true_original']\n",
        "\n",
        "print(f\"\\nüìä Prediction Statistics:\")\n",
        "print(f\"   Predictions shape: {y_pred.shape}\")\n",
        "print(f\"   True values shape: {y_true.shape}\")\n",
        "print(f\"   Prediction range: ${y_pred.min():.2f} - ${y_pred.max():.2f}\")\n",
        "print(f\"   True value range: ${y_true.min():.2f} - ${y_true.max():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Plot Predictions vs Actual\n",
        "print(\"üìà Plotting predictions vs actual...\")\n",
        "target_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "\n",
        "# Plot predictions vs actual for each target\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(target_columns):\n",
        "    if i < len(axes):\n",
        "        axes[i].scatter(y_true[:, i], y_pred[:, i], alpha=0.6)\n",
        "        axes[i].plot([y_true[:, i].min(), y_true[:, i].max()], \n",
        "                    [y_true[:, i].min(), y_true[:, i].max()], 'r--', lw=2)\n",
        "        axes[i].set_xlabel(f'Actual {col}')\n",
        "        axes[i].set_ylabel(f'Predicted {col}')\n",
        "        axes[i].set_title(f'{col} - Predictions vs Actual')\n",
        "        axes[i].grid(True)\n",
        "\n",
        "# Remove empty subplot\n",
        "if len(target_columns) < len(axes):\n",
        "    fig.delaxes(axes[-1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Plot Prediction Errors\n",
        "print(\"üìà Plotting prediction errors...\")\n",
        "\n",
        "# Calculate errors\n",
        "errors = y_pred - y_true\n",
        "\n",
        "# Plot error distributions\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(target_columns):\n",
        "    if i < len(axes):\n",
        "        axes[i].hist(errors[:, i], bins=30, alpha=0.7, edgecolor='black')\n",
        "        axes[i].set_xlabel(f'Error ({col})')\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "        axes[i].set_title(f'{col} - Error Distribution')\n",
        "        axes[i].grid(True)\n",
        "        axes[i].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Remove empty subplot\n",
        "if len(target_columns) < len(axes):\n",
        "    fig.delaxes(axes[-1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Advanced Features and On-Demand Processing\n",
        "\n",
        "Demonstrate advanced features like on-demand data generation and future predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: On-Demand Data Generation\n",
        "print(\"üîÑ Demonstrating on-demand data generation...\")\n",
        "\n",
        "# Get data for a specific time range\n",
        "start_time = \"2021-01-15 00:00:00\"\n",
        "end_time = \"2021-01-15 23:59:59\"\n",
        "\n",
        "print(f\"üìÖ Requesting data for range: {start_time} to {end_time}\")\n",
        "\n",
        "range_data = organizer.get_data_in_range(\n",
        "    start_time=start_time,\n",
        "    end_time=end_time,\n",
        "    scaled=True\n",
        ")\n",
        "\n",
        "if range_data is not None:\n",
        "    print(f\"‚úÖ Range data generated:\")\n",
        "    print(f\"   Available keys: {list(range_data.keys())}\")\n",
        "    print(f\"   X shape: {range_data['X_scaled'].shape}\")\n",
        "    print(f\"   y shape: {range_data['y_scaled'].shape}\")\n",
        "    \n",
        "    # Make predictions on this range\n",
        "    range_predictions = model.predict(range_data['X_scaled'], verbose=0)\n",
        "    print(f\"   Predictions shape: {range_predictions.shape}\")\n",
        "    \n",
        "    # Show sample predictions\n",
        "    print(f\"\\nüìä Sample Predictions:\")\n",
        "    for i in range(min(3, len(range_predictions))):\n",
        "        print(f\"   Sample {i+1}: {range_predictions[i]}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No data available for the specified range\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Future Predictions\n",
        "print(\"üîÆ Making future predictions...\")\n",
        "\n",
        "# Use the last sequence from test data for prediction\n",
        "last_sequence = X_test_scaled[-1:]  # Shape: (1, sequence_length, features)\n",
        "print(f\"   Using last sequence shape: {last_sequence.shape}\")\n",
        "\n",
        "# Make prediction\n",
        "future_prediction = model.predict(last_sequence, verbose=0)\n",
        "print(f\"   Prediction shape: {future_prediction.shape}\")\n",
        "\n",
        "# Inverse transform prediction to original scale\n",
        "future_prediction_original = scaler_y.inverse_transform(future_prediction)\n",
        "print(f\"   Original scale prediction shape: {future_prediction_original.shape}\")\n",
        "\n",
        "# Display prediction results\n",
        "target_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "print(f\"\\nüìà Future Prediction (Next Candle):\")\n",
        "for i, col in enumerate(target_columns):\n",
        "    print(f\"   {col}: ${future_prediction_original[0, i]:.2f}\")\n",
        "\n",
        "# Compare with last known values\n",
        "last_known = scaler_y.inverse_transform(y_test_scaled[-1:])[0]\n",
        "print(f\"\\nüìä Last Known Values:\")\n",
        "for i, col in enumerate(target_columns):\n",
        "    print(f\"   {col}: ${last_known[i]:.2f}\")\n",
        "\n",
        "# Calculate changes\n",
        "print(f\"\\nüìä Predicted Changes:\")\n",
        "for i, col in enumerate(target_columns):\n",
        "    change = future_prediction_original[0, i] - last_known[i]\n",
        "    change_pct = (change / last_known[i]) * 100\n",
        "    print(f\"   {col}: {change:+.2f} ({change_pct:+.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Simplified Method Summary\n",
        "\n",
        "Summary of the simplified OHLCV + Minutes_of_day approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: Simplified Method Summary\n",
        "print(\"üìä SIMPLIFIED METHOD SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\n‚úÖ Current Implementation:\")\n",
        "print(f\"   Input features: 5 (Open, High, Low, Close, Volume)\")\n",
        "print(f\"   Target values: 5 (Open, High, Low, Close, Volume)\")\n",
        "print(f\"   Sequence length: {config.sequence_length} timesteps\")\n",
        "print(f\"   Data shape: X={X_train_scaled.shape}, y={y_train_scaled.shape}\")\n",
        "\n",
        "print(f\"\\nüìä Feature Statistics:\")\n",
        "print(f\"   X range: {X_train_scaled.min():.4f} to {X_train_scaled.max():.4f}\")\n",
        "print(f\"   y range: {y_train_scaled.min():.4f} to {y_train_scaled.max():.4f}\")\n",
        "\n",
        "print(f\"\\nüéØ Benefits of Simplified Approach:\")\n",
        "print(f\"   ‚úÖ Reduced complexity - only essential OHLCV data\")\n",
        "print(f\"   ‚úÖ Price-based patterns - OHLCV captures market dynamics\")\n",
        "print(f\"   ‚úÖ Memory efficient - minimal feature engineering\")\n",
        "print(f\"   ‚úÖ Fast training - fewer parameters to optimize\")\n",
        "print(f\"   ‚úÖ Easy to understand - clear input/output relationship\")\n",
        "\n",
        "print(f\"\\nüìà Model Performance:\")\n",
        "print(f\"   Test Loss: {evaluation_results['test_loss']:.6f}\")\n",
        "print(f\"   Test MAE: {evaluation_results['test_mae']:.6f}\")\n",
        "print(f\"   Test MAPE: {evaluation_results['test_mape']:.2f}%\")\n",
        "print(f\"   RMSE: {evaluation_results['rmse']:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Final Summary and Results\n",
        "\n",
        "Display comprehensive results and summary of the prediction system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 14: Final Summary\n",
        "print(\"üéâ COMPREHENSIVE CRYPTOCURRENCY PREDICTION COMPLETED!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìä FINAL RESULTS:\")\n",
        "print(f\"   Symbol: {config.symbol}\")\n",
        "print(f\"   Timeframe: {config.timeframe}\")\n",
        "print(f\"   Data period: {config.start_time} to {config.end_time}\")\n",
        "print(f\"   Total sequences: {feature_info['total_sequences']}\")\n",
        "print(f\"   Model parameters: {model.count_params():,}\")\n",
        "print(f\"   Training epochs: {EPOCHS}\")\n",
        "print(f\"   Final test loss: {evaluation_results['test_loss']:.6f}\")\n",
        "print(f\"   Final test MAE: {evaluation_results['test_mae']:.6f}\")\n",
        "print(f\"   Final test MAPE: {evaluation_results['test_mape']:.2f}%\")\n",
        "\n",
        "print(f\"\\nüîß FEATURES USED:\")\n",
        "for group_name, features in feature_groups.items():\n",
        "    print(f\"   {group_name.capitalize()}: {len(features)} features\")\n",
        "    for feature in features:\n",
        "        print(f\"     - {feature}\")\n",
        "\n",
        "print(f\"\\nüí° ADVANCED FEATURES DEMONSTRATED:\")\n",
        "print(f\"   ‚úÖ Integrated data management with BinanceDataOrganizer\")\n",
        "print(f\"   ‚úÖ On-demand data generation and processing\")\n",
        "print(f\"   ‚úÖ Grouped normalization for different feature types\")\n",
        "print(f\"   ‚úÖ Memory-efficient training and evaluation\")\n",
        "print(f\"   ‚úÖ Comprehensive visualization and analysis\")\n",
        "print(f\"   ‚úÖ Model persistence and configuration management\")\n",
        "print(f\"   ‚úÖ Advanced plotting and error analysis\")\n",
        "\n",
        "print(f\"\\nüéØ The comprehensive system provides a complete solution\")\n",
        "print(f\"   for cryptocurrency prediction with advanced normalization,\")\n",
        "print(f\"   memory-efficient data management, and comprehensive analysis!\")\n",
        "\n",
        "print(f\"\\n‚úÖ All tasks completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "crypto_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
