{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cryptocurrency Price Prediction with LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from utils import download_crypto_data, scale_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GLOBAL CONFIGURATION PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Data Configuration\n",
    "SYMBOL = \"BTCUSDT\"                    # Trading pair symbol\n",
    "INTERVAL = \"5m\"                       # Time interval (1m, 5m, 15m, 1h, 4h, 1d)\n",
    "DATA_FROM = \"2021 01\"                 # Start date for data download (YYYY MM)\n",
    "DATA_TO = \"2021 01\"                   # End date for data download (YYYY MM)\n",
    "\n",
    "# Model Configuration\n",
    "LSTM_UNITS = 50                       # Number of LSTM units\n",
    "DROPOUT_RATE = 0.2                    # Dropout rate for regularization\n",
    "EPOCHS = 50                           # Number of training epochs\n",
    "BATCH_SIZE = 32                       # Batch size for training\n",
    "TEST_SIZE = 0.2                       # Proportion of data for testing (0.2 = 20%)\n",
    "\n",
    "# Feature Engineering Configuration\n",
    "SEQUENCE_LENGTH = 15                  # Number of previous candles to use for prediction\n",
    "\n",
    "# Prediction Configuration\n",
    "PREDICTION_LENGTH = 1                 # Number of future steps to predict\n",
    "SAMPLE_SIZE = 200                     # Number of samples for demonstration\n",
    "\n",
    "# Visualization Configuration\n",
    "MAX_POINTS = 2000                     # Maximum points to plot for performance\n",
    "MAX_SAMPLES = 1000                    # Maximum samples for prediction plots\n",
    "\n",
    "print(\"âœ… Global configuration loaded successfully!\")\n",
    "print(f\"ðŸ“Š Data: {SYMBOL} {INTERVAL} from {DATA_FROM} to {DATA_TO}\")\n",
    "print(f\"ðŸ§  Model: {LSTM_UNITS} units, {EPOCHS} epochs, batch size {BATCH_SIZE}\")\n",
    "print(f\"ðŸ”§ Features: {SEQUENCE_LENGTH} candle sequence, no rolling windows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "This section contains functions to download and load cryptocurrency data from Binance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_range(from_date, to_date):\n",
    "    \"\"\"\n",
    "    Generate years and months from date range.\n",
    "    \n",
    "    Args:\n",
    "        from_date (str): Start date in \"YYYY MM\" format\n",
    "        to_date (str): End date in \"YYYY MM\" format\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (years, months) - lists of years and months to download\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    start_year, start_month = from_date.split()\n",
    "    end_year, end_month = to_date.split()\n",
    "    \n",
    "    start_year = int(start_year)\n",
    "    start_month = int(start_month)\n",
    "    end_year = int(end_year)\n",
    "    end_month = int(end_month)\n",
    "    \n",
    "    years = []\n",
    "    months = []\n",
    "    \n",
    "    current_year = start_year\n",
    "    current_month = start_month\n",
    "    \n",
    "    while (current_year < end_year) or (current_year == end_year and current_month <= end_month):\n",
    "        year_str = str(current_year)\n",
    "        month_str = f\"{current_month:02d}\"\n",
    "        \n",
    "        if year_str not in years:\n",
    "            years.append(year_str)\n",
    "        if month_str not in months:\n",
    "            months.append(month_str)\n",
    "        \n",
    "        # Move to next month\n",
    "        if current_month == 12:\n",
    "            current_year += 1\n",
    "            current_month = 1\n",
    "        else:\n",
    "            current_month += 1\n",
    "    \n",
    "    return years, months\n",
    "\n",
    "\n",
    "def load_data_by_date_range(symbol, interval, from_date, to_date):\n",
    "    \"\"\"\n",
    "    Load data for a specific date range, combining into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): Trading pair symbol\n",
    "        interval (str): Time interval\n",
    "        from_date (str): Start date in \"YYYY MM\" format\n",
    "        to_date (str): End date in \"YYYY MM\" format\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with all data in the date range\n",
    "    \"\"\"\n",
    "    years, months = generate_date_range(from_date, to_date)\n",
    "    print(f\"Loading data from {from_date} to {to_date}\")\n",
    "    print(f\"Years: {years}\")\n",
    "    print(f\"Months: {months}\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for year in years:\n",
    "        print(f\"Loading data for year {year}...\")\n",
    "        year_data = load_multiple_months_data(symbol, interval, year, months)\n",
    "        if year_data is not None:\n",
    "            all_data.append(year_data)\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        combined_df = combined_df.sort_values('Open time').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Total combined data shape: {combined_df.shape}\")\n",
    "        print(f\"Date range: {combined_df['Open time'].min()} to {combined_df['Open time'].max()}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No data loaded successfully\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bitcoin data using common download function\n",
    "btc_data = download_crypto_data(\n",
    "    symbol=SYMBOL,\n",
    "    interval=INTERVAL, \n",
    "    data_from=DATA_FROM,\n",
    "    data_to=DATA_TO,\n",
    "    max_rows=50000,  # Limit for memory efficiency\n",
    "    # Uses Binance Vision only\n",
    ")\n",
    "\n",
    "if btc_data is not None and not btc_data.empty:\n",
    "    print(f\"\\nðŸ“‹ Sample Data:\")\n",
    "    display(btc_data.head())\n",
    "else:\n",
    "    print(\"âŒ No data available for processing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This section handles feature engineering, scaling, and data preparation for LSTM training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_features(df):\n",
    "    \"\"\"\n",
    "    Create time series features for each candle with proper 5-minute interval encoding.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with OHLCV data\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with time series features\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”§ Creating time series features...\")\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Time-based features for 5-minute intervals\n",
    "    # 5-minute intervals = 288 intervals per day (24 * 60 / 5)\n",
    "    print(\"â° Adding time-based features...\")\n",
    "    data['Hour'] = data['Open time'].dt.hour\n",
    "    data['Minute'] = data['Open time'].dt.minute\n",
    "    \n",
    "    # Calculate the 5-minute interval number within the day (0-287)\n",
    "    data['Interval_5min'] = data['Hour'] * 12 + data['Minute'] // 5\n",
    "    \n",
    "    # Cyclical encoding for 5-minute intervals\n",
    "    data['Interval_sin'] = np.sin(2 * np.pi * data['Interval_5min'] / 288)\n",
    "    data['Interval_cos'] = np.cos(2 * np.pi * data['Interval_5min'] / 288)\n",
    "    \n",
    "    # Also keep hour-based features for broader patterns\n",
    "    data['Hour_sin'] = np.sin(2 * np.pi * data['Hour'] / 24)\n",
    "    data['Hour_cos'] = np.cos(2 * np.pi * data['Hour'] / 24)\n",
    "    \n",
    "    # Price-based features\n",
    "    print(\"ðŸ’° Adding price-based features...\")\n",
    "    data['Price_Range'] = data['High'] - data['Low']\n",
    "    data['Price_Change'] = data['Close'] - data['Open']\n",
    "    data['Price_Change_Pct'] = (data['Close'] - data['Open']) / data['Open']\n",
    "    \n",
    "    # Volume features\n",
    "    data['Volume_MA_5'] = data['Volume'].rolling(window=5).mean()\n",
    "    data['Volume_MA_10'] = data['Volume'].rolling(window=10).mean()\n",
    "    \n",
    "    print(f\"âœ… Time series features created! Shape: {data.shape}\")\n",
    "    \n",
    "    # Show time feature examples\n",
    "    print(f\"\\nâ° Time Feature Examples:\")\n",
    "    print(f\"   Hour range: {data['Hour'].min()}-{data['Hour'].max()}\")\n",
    "    print(f\"   5-min interval range: {data['Interval_5min'].min()}-{data['Interval_5min'].max()}\")\n",
    "    print(f\"   Interval_sin range: {data['Interval_sin'].min():.3f} to {data['Interval_sin'].max():.3f}\")\n",
    "    print(f\"   Interval_cos range: {data['Interval_cos'].min():.3f} to {data['Interval_cos'].max():.3f}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_sliding_windows(data, sequence_length=10, target_cols=['Open', 'High', 'Low', 'Close', 'Volume']):\n",
    "    \"\"\"\n",
    "    Create sliding windows for time series prediction.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with features\n",
    "        sequence_length (int): Number of previous candles to use for prediction\n",
    "        target_cols (list): Columns to predict\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X, y, feature_cols) - Features and targets for LSTM training\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”„ Creating sliding windows (sequence length: {sequence_length})...\")\n",
    "    \n",
    "    # Select feature columns (exclude time and target columns)\n",
    "    feature_cols = [col for col in data.columns if col not in ['Open time', 'Close time'] + target_cols]\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    data_clean = data.dropna()\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(sequence_length, len(data_clean)):\n",
    "        # Input: sequence_length previous candles\n",
    "        X.append(data_clean[feature_cols].iloc[i-sequence_length:i].values)\n",
    "        \n",
    "        # Target: next candle's OHLCV values\n",
    "        y.append(data_clean[target_cols].iloc[i].values)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"âœ… Sliding windows created!\")\n",
    "    print(f\"ðŸ“Š X shape: {X.shape} (samples, sequence_length, features)\")\n",
    "    print(f\"ðŸ“Š y shape: {y.shape} (samples, targets)\")\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "\n",
    "def preprocess_data_for_lstm(df, test_size=TEST_SIZE):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for LSTM training.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with OHLCV data\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test, feature_cols)\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ Starting LSTM prediction preprocessing...\")\n",
    "    \n",
    "    # Step 1: Create time series features\n",
    "    data_with_features = create_time_series_features(df)\n",
    "    \n",
    "    # Step 2: Create sliding windows\n",
    "    X, y, feature_cols = create_sliding_windows(data_with_features, SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Step 3: Split data\n",
    "    split_index = int(len(X) * (1 - test_size))\n",
    "    \n",
    "    X_train = X[:split_index]\n",
    "    X_test = X[split_index:]\n",
    "    y_train = y[:split_index]\n",
    "    y_test = y[split_index:]\n",
    "    \n",
    "    print(f\"âœ‚ï¸ Data split completed!\")\n",
    "    print(f\"ðŸ“Š Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"ðŸ“Š Test samples: {X_test.shape[0]}\")\n",
    "    print(f\"ðŸ“Š Sequence length: {X_train.shape[1]}\")\n",
    "    print(f\"ðŸ“Š Features per timestep: {X_train.shape[2]}\")\n",
    "    print(f\"ðŸ“Š Target variables: {y_train.shape[1]}\")\n",
    "    \n",
    "    # Show data structure\n",
    "    print(f\"\\nðŸ“Š DATA STRUCTURE DEMONSTRATION:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"\\nðŸ” Sample 1:\")\n",
    "    print(f\"   Input: {X_train.shape[1]} previous candles â†’ Predict next candle\")\n",
    "    print(f\"   Features per candle: {X_train.shape[2]}\")\n",
    "    print(f\"   Target: {y_train.shape[1]} OHLCV values\")\n",
    "    \n",
    "    # Show the last candle's features\n",
    "    last_candle_features = X_train[0, -1, :]  # Last timestep of first sample\n",
    "    print(f\"\\n   Last candle features (first 10):\")\n",
    "    for j in range(min(10, len(feature_cols))):\n",
    "        print(f\"     {feature_cols[j]}: {last_candle_features[j]:.4f}\")\n",
    "    if len(feature_cols) > 10:\n",
    "        print(f\"     ... and {len(feature_cols) - 10} more features\")\n",
    "    \n",
    "    # Show target\n",
    "    print(f\"\\n   Target (next candle OHLCV):\")\n",
    "    target_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    for j, target in enumerate(target_cols):\n",
    "        print(f\"     {target}: {y_train[0, j]:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ This means:\")\n",
    "    print(f\"   - Each training sample uses {X_train.shape[1]} consecutive candles\")\n",
    "    print(f\"   - Each candle has {X_train.shape[2]} features\")\n",
    "    print(f\"   - Model predicts the next candle's {y_train.shape[1]} values\")\n",
    "    print(f\"   - Total training samples: {X_train.shape[0]}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_time_series_data(X_train, X_test, y_train, y_test, feature_cols):\n",
    "    \"\"\"\n",
    "    Scale the time series data for LSTM training.\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_test: Training and test features (samples, timesteps, features)\n",
    "        y_train, y_test: Training and test targets\n",
    "        feature_cols: List of feature column names\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_X, scaler_y)\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”¢ Scaling time series data...\")\n",
    "    \n",
    "    # Reshape for scaling (samples * timesteps, features)\n",
    "    X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "    X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "    \n",
    "    # Scale features\n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train_reshaped)\n",
    "    X_test_scaled = scaler_X.transform(X_test_reshaped)\n",
    "    \n",
    "    # Reshape back to (samples, timesteps, features)\n",
    "    X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
    "    X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
    "    \n",
    "    # Scale targets\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "    \n",
    "    print(\"âœ… Scaling completed!\")\n",
    "    print(f\"ðŸ“Š Scaled X_train shape: {X_train_scaled.shape}\")\n",
    "    print(f\"ðŸ“Š Scaled y_train shape: {y_train_scaled.shape}\")\n",
    "    \n",
    "    # Show scaled data examples\n",
    "    print(f\"\\nðŸ“Š SCALED DATA EXAMPLES:\")\n",
    "    print(f\"   First sample, last candle features (first 10):\")\n",
    "    for j in range(min(10, len(feature_cols))):\n",
    "        print(f\"     {feature_cols[j]}: {X_train_scaled[0, -1, j]:.4f}\")\n",
    "    \n",
    "    print(f\"   First sample target (OHLCV):\")\n",
    "    target_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    for j, target in enumerate(target_cols):\n",
    "        print(f\"     {target}: {y_train_scaled[0, j]:.4f}\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_X, scaler_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if btc_data is not None:\n",
    "    X_train, X_test, y_train, y_test, feature_cols = preprocess_data_for_lstm(btc_data)\n",
    "    print(\"âœ… Data preprocessing completed successfully!\")\n",
    "    \n",
    "    # Show processed data tables\n",
    "    print(\"\\nðŸ“Š PROCESSED DATA TABLES:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nðŸ“‹ Training Features (First 10 samples, last candle):\")\n",
    "    print(\"   Shape:\", X_train.shape)\n",
    "    print(\"   First 10 samples, last candle features (first 10):\")\n",
    "    for i in range(min(10, len(X_train))):\n",
    "        print(f\"     Sample {i+1}: {X_train[i, -1, :10]}\")\n",
    "    \n",
    "    print(\"\\nðŸ“‹ Test Features (First 10 samples, last candle):\")\n",
    "    print(\"   Shape:\", X_test.shape)\n",
    "    print(\"   First 10 samples, last candle features (first 10):\")\n",
    "    for i in range(min(10, len(X_test))):\n",
    "        print(f\"     Sample {i+1}: {X_test[i, -1, :10]}\")\n",
    "    \n",
    "    print(\"\\nðŸ“‹ Training Targets (First 10 samples):\")\n",
    "    print(\"   Shape:\", y_train.shape)\n",
    "    print(\"   First 10 samples targets (OHLCV):\")\n",
    "    for i in range(min(10, len(y_train))):\n",
    "        print(f\"     Sample {i+1}: {y_train[i]}\")\n",
    "    \n",
    "    print(\"\\nðŸ“‹ Test Targets (First 10 samples):\")\n",
    "    print(\"   Shape:\", y_test.shape)\n",
    "    print(\"   First 10 samples targets (OHLCV):\")\n",
    "    for i in range(min(10, len(y_test))):\n",
    "        print(f\"     Sample {i+1}: {y_test[i]}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Data Shapes Summary:\")\n",
    "    print(f\"Training features: {X_train.shape} (samples, sequence_length, features)\")\n",
    "    print(f\"Test features: {X_test.shape} (samples, sequence_length, features)\")\n",
    "    print(f\"Training targets: {y_train.shape} (samples, targets)\")\n",
    "    print(f\"Test targets: {y_test.shape} (samples, targets)\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Feature Information:\")\n",
    "    print(f\"   Total features: {len(feature_cols)}\")\n",
    "    print(f\"   Feature names (first 10): {feature_cols[:10]}\")\n",
    "    print(f\"   Target variables: ['Open', 'High', 'Low', 'Close', 'Volume']\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Data Range Information:\")\n",
    "    print(f\"   X_train min: {X_train.min():.4f}, max: {X_train.max():.4f}\")\n",
    "    print(f\"   y_train min: {y_train.min():.4f}, max: {y_train.max():.4f}\")\n",
    "    print(f\"   X_test min: {X_test.min():.4f}, max: {X_test.max():.4f}\")\n",
    "    print(f\"   y_test min: {y_test.min():.4f}, max: {y_test.max():.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No data available for preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload utils module to get updated scale_data function\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Remove utils from cache\n",
    "if 'utils' in sys.modules:\n",
    "    del sys.modules['utils']\n",
    "\n",
    "# Remove individual utils modules\n",
    "for module_name in list(sys.modules.keys()):\n",
    "    if module_name.startswith('utils.'):\n",
    "        del sys.modules[module_name]\n",
    "\n",
    "# Re-import utils\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Re-import scale_data\n",
    "from utils import scale_data\n",
    "\n",
    "print('âœ… Utils module reloaded successfully!')\n",
    "print('âœ… scale_data function updated!')\n",
    "\n",
    "# Store original data before scaling (needed for evaluation)\n",
    "if 'X_train' in locals() and 'X_test' in locals() and 'y_train' in locals() and 'y_test' in locals():\n",
    "    X_train_orig = X_train.copy()\n",
    "    X_test_orig = X_test.copy()\n",
    "    y_train_orig = y_train.copy()\n",
    "    y_test_orig = y_test.copy()\n",
    "    print('âœ… Original data stored for evaluation!')\n",
    "else:\n",
    "    print('âŒ No data available for storing originals')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data for LSTM training\n",
    "if 'X_train' in locals() and 'X_test' in locals() and 'y_train' in locals() and 'y_test' in locals():\n",
    "    X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler_X, scaler_y = scale_data(X_train, X_test, y_train, y_test)\n",
    "    print(\"âœ… Data scaling completed successfully!\")\n",
    "else:\n",
    "    print(\"âŒ No data available for scaling. Please run preprocessing first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "This section defines the LSTM model architecture for multi-output time series prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape, output_shape, units=50, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Build and compile an LSTM model for multi-output time series prediction.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): The shape of the input data (timesteps, num_features)\n",
    "        output_shape (int): The number of output features\n",
    "        units (int): The number of units in the LSTM layer\n",
    "        dropout_rate (float): The dropout rate for regularization\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: The compiled LSTM model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(units=units, activation='relu', input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(units=output_shape)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model(input_shape, output_shape):\n",
    "    \"\"\"\n",
    "    Create and return a configured LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Input shape for the model\n",
    "        output_shape (int): Number of output features\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled LSTM model\n",
    "    \"\"\"\n",
    "    model = build_lstm_model(input_shape, output_shape)\n",
    "    \n",
    "    print(\"LSTM Model Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bitcoin data using common download function\n",
    "btc_data = download_crypto_data(\n",
    "    symbol=SYMBOL,\n",
    "    interval=INTERVAL, \n",
    "    data_from=DATA_FROM,\n",
    "    data_to=DATA_TO,\n",
    "    max_rows=50000,  # Limit for memory efficiency\n",
    "    # Uses Binance Vision only\n",
    ")\n",
    "\n",
    "if btc_data is not None and not btc_data.empty:\n",
    "    print(f\"\\\\nðŸ“‹ Sample Data:\")\n",
    "    display(btc_data.head())\n",
    "else:\n",
    "    print(\"âŒ No data available for processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train_scaled' in locals() and 'y_train_scaled' in locals():\n",
    "    input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "    output_shape = y_train_scaled.shape[1]\n",
    "    \n",
    "    model = create_model(input_shape, output_shape)\n",
    "    print(\"Model created successfully!\")\n",
    "else:\n",
    "    print(\"Training data not available. Please run preprocessing first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "This section handles model training, evaluation, and visualization of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train the LSTM model and return training history.\n",
    "    \n",
    "    Args:\n",
    "        model: The LSTM model to train\n",
    "        X_train: Training features\n",
    "        y_train: Training targets\n",
    "        X_test: Test features\n",
    "        y_test: Test targets\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.callbacks.History: Training history\n",
    "    \"\"\"\n",
    "    print(\"Starting model training...\")\n",
    "    \n",
    "    # Clean data to remove any infinite values\n",
    "    train_mask = np.all(np.isfinite(X_train), axis=(1, 2)) & np.all(np.isfinite(y_train), axis=1)\n",
    "    test_mask = np.all(np.isfinite(X_test), axis=(1, 2)) & np.all(np.isfinite(y_test), axis=1)\n",
    "    \n",
    "    X_train_clean = X_train[train_mask]\n",
    "    y_train_clean = y_train[train_mask]\n",
    "    X_test_clean = X_test[test_mask]\n",
    "    y_test_clean = y_test[test_mask]\n",
    "    \n",
    "    print(f\"Training on {len(X_train_clean)} samples\")\n",
    "    print(f\"Testing on {len(X_test_clean)} samples\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_clean, y_train_clean,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss = model.evaluate(X_test_clean, y_test_clean, verbose=0)\n",
    "    print(f\"Test Loss: {test_loss[0]:.6f}\")\n",
    "    print(f\"Test MAE: {test_loss[1]:.6f}\")\n",
    "    \n",
    "    return history, X_test_clean, y_test_clean\n",
    "\n",
    "\n",
    "def evaluate_model_performance(model, X_test, y_test, scaler_y, y_test_orig):\n",
    "    \"\"\"\n",
    "    Evaluate model performance and create visualizations.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        X_test: Test features\n",
    "        y_test: Test targets (scaled)\n",
    "        scaler_y: Target scaler for inverse transformation\n",
    "        y_test_orig: Original test targets (unscaled)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Performance metrics\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    predictions_scaled = model.predict(X_test)\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "    y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "    \n",
    "    # Calculate MSE for each target\n",
    "    target_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    mse_scores = {}\n",
    "    \n",
    "    print(\"Model Performance (MSE):\")\n",
    "    for i, col in enumerate(target_columns):\n",
    "        mse = mean_squared_error(y_test_actual[:, i], predictions[:, i])\n",
    "        mse_scores[col] = mse\n",
    "        print(f\"{col}: {mse:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'actual': y_test_actual,\n",
    "        'mse_scores': mse_scores,\n",
    "        'target_columns': target_columns\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss.\n",
    "    \n",
    "    Args:\n",
    "        history: Training history from model.fit()\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions_vs_actual(predictions, actual, target_columns, max_samples=1000):\n",
    "    \"\"\"\n",
    "    Plot predictions vs actual values for each target.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted values\n",
    "        actual: Actual values\n",
    "        target_columns: List of target column names\n",
    "        max_samples: Maximum number of samples to plot\n",
    "    \"\"\"\n",
    "    n_samples = min(len(predictions), max_samples)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(target_columns), 1, figsize=(15, 3 * len(target_columns)))\n",
    "    if len(target_columns) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(target_columns):\n",
    "        axes[i].plot(actual[:n_samples, i], label=f'Actual {col}', alpha=0.7)\n",
    "        axes[i].plot(predictions[:n_samples, i], label=f'Predicted {col}', alpha=0.7)\n",
    "        axes[i].set_title(f'{col} - Actual vs Predicted')\n",
    "        axes[i].set_xlabel('Time Steps')\n",
    "        axes[i].set_ylabel(col)\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in locals() and 'X_train_scaled' in locals() and 'y_train_scaled' in locals():\n",
    "    history, X_test_clean, y_test_clean = train_model(model, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, epochs=50)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    results = evaluate_model_performance(model, X_test_clean, y_test_clean, scaler_y, y_test_orig)\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    plot_predictions_vs_actual(\n",
    "        results['predictions'], \n",
    "        results['actual'], \n",
    "        results['target_columns']\n",
    "    )\n",
    "    \n",
    "    print(\"Training and evaluation completed!\")\n",
    "else:\n",
    "    print(\"Model or training data not available. Please run previous cells first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Functions\n",
    "\n",
    "This section provides functions to make predictions on new data using the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_step(model, new_data_df, scaler_X, scaler_y, timesteps=1):\n",
    "    \"\"\"\n",
    "    Make predictions on new data using the trained LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        new_data_df (pd.DataFrame): DataFrame containing historical data for feature calculation\n",
    "        scaler_X: Fitted scaler for input features\n",
    "        scaler_y: Fitted scaler for target variables\n",
    "        timesteps (int): Number of timesteps the LSTM expects\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Predictions for the next time step\n",
    "    \"\"\"\n",
    "    if new_data_df is None or new_data_df.empty:\n",
    "        print(\"Error: No new data provided for prediction.\")\n",
    "        return None\n",
    "    \n",
    "    # Create features using the same preprocessing as training\n",
    "    features_df = create_time_series_features(new_data_df)\n",
    "    \n",
    "    # Remove original features and time columns (keep only engineered ones)\n",
    "    # Use the same feature selection as in create_sliding_windows function\n",
    "    target_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    feature_cols = [col for col in features_df.columns if col not in ['Open time', 'Close time'] + target_cols]\n",
    "    features_df = features_df[feature_cols]\n",
    "    \n",
    "    # Get the last valid row of features\n",
    "    latest_features = features_df.dropna().iloc[[-1]].copy()\n",
    "    \n",
    "    if latest_features.empty:\n",
    "        print(\"Error: Not enough valid data to calculate features for prediction.\")\n",
    "        return None\n",
    "    \n",
    "    # Scale the features\n",
    "    latest_features_scaled = scaler_X.transform(latest_features)\n",
    "    \n",
    "    # Reshape for LSTM input\n",
    "    latest_features_lstm = latest_features_scaled.reshape((1, timesteps, latest_features_scaled.shape[1]))\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions_scaled = model.predict(latest_features_lstm)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    target_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    prediction_df = pd.DataFrame(predictions, columns=target_columns)\n",
    "    \n",
    "    return prediction_df\n",
    "\n",
    "\n",
    "def generate_sample_data(original_data, n_samples=100, noise_factor=0.001):\n",
    "    \"\"\"\n",
    "    Generate sample data for testing predictions.\n",
    "    \n",
    "    Args:\n",
    "        original_data: Original DataFrame to use as template\n",
    "        n_samples: Number of samples to generate\n",
    "        noise_factor: Amount of noise to add (as fraction of standard deviation)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Generated sample data\n",
    "    \"\"\"\n",
    "    # Take the last n_samples from original data\n",
    "    sample_data = original_data.tail(n_samples).copy()\n",
    "    \n",
    "    # Add small amount of noise to make it different\n",
    "    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "        noise = np.random.randn(len(sample_data)) * sample_data[col].std() * noise_factor\n",
    "        sample_data[col] = sample_data[col] + noise\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "\n",
    "def demonstrate_prediction(model, scaler_X, scaler_y, original_data):\n",
    "    \"\"\"\n",
    "    Demonstrate prediction functionality with sample data.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        scaler_X: Fitted scaler for input features\n",
    "        scaler_y: Fitted scaler for target variables\n",
    "        original_data: Original data to use for generating samples\n",
    "    \"\"\"\n",
    "    print(\"Generating sample data for prediction demonstration...\")\n",
    "    \n",
    "    # Generate sample data\n",
    "    sample_data = generate_sample_data(original_data, n_samples=100)\n",
    "    \n",
    "    print(f\"Sample data shape: {sample_data.shape}\")\n",
    "    print(\"Last 5 rows of sample data:\")\n",
    "    display(sample_data[['Open time', 'Open', 'High', 'Low', 'Close', 'Volume']].tail())\n",
    "    \n",
    "    # Make prediction\n",
    "    print(\"\\nMaking prediction for next time step...\")\n",
    "    prediction = predict_next_step(model, sample_data, scaler_X, scaler_y)\n",
    "    \n",
    "    if prediction is not None:\n",
    "        print(\"Predicted values for next time step:\")\n",
    "        display(prediction)\n",
    "        \n",
    "        # Show the actual next values for comparison (if available)\n",
    "        if len(original_data) > len(sample_data):\n",
    "            actual_next = original_data.iloc[len(sample_data):len(sample_data)+1][['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "            print(\"\\nActual values for comparison:\")\n",
    "            display(actual_next)\n",
    "    else:\n",
    "        print(\"Failed to generate prediction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate prediction functionality\n",
    "if 'model' in locals() and 'scaler_X' in locals() and 'scaler_y' in locals() and 'btc_data' in locals():\n",
    "    demonstrate_prediction(model, scaler_X, scaler_y, btc_data)\n",
    "else:\n",
    "    print(\"Model, scalers, or data not available. Please run training first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Visualization Functions\n",
    "\n",
    "This section provides additional visualization utilities for analyzing the data and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price_data(df, title=\"Cryptocurrency Price Data\", max_points=2000):\n",
    "    \"\"\"\n",
    "    Plot the original price data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLCV data\n",
    "        title: Plot title\n",
    "        max_points: Maximum number of points to plot\n",
    "    \"\"\"\n",
    "    n_points = min(len(df), max_points)\n",
    "    data_subset = df.tail(n_points)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Price subplot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(data_subset['Open time'], data_subset['Close'], label='Close Price', alpha=0.8)\n",
    "    plt.title('Close Price Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price (USDT)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Volume subplot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(data_subset['Open time'], data_subset['Volume'], label='Volume', color='orange', alpha=0.8)\n",
    "    plt.title('Volume Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Volume')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # OHLC subplot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(data_subset['Open time'], data_subset['Open'], label='Open', alpha=0.7)\n",
    "    plt.plot(data_subset['Open time'], data_subset['High'], label='High', alpha=0.7)\n",
    "    plt.plot(data_subset['Open time'], data_subset['Low'], label='Low', alpha=0.7)\n",
    "    plt.plot(data_subset['Open time'], data_subset['Close'], label='Close', alpha=0.7)\n",
    "    plt.title('OHLC Prices')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price (USDT)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Price distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(data_subset['Close'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Close Price Distribution')\n",
    "    plt.xlabel('Price (USDT)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_prediction_errors(predictions, actual, target_columns):\n",
    "    \"\"\"\n",
    "    Plot prediction errors for each target variable.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted values\n",
    "        actual: Actual values\n",
    "        target_columns: List of target column names\n",
    "    \"\"\"\n",
    "    errors = actual - predictions\n",
    "    \n",
    "    fig, axes = plt.subplots(len(target_columns), 1, figsize=(15, 3 * len(target_columns)))\n",
    "    if len(target_columns) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(target_columns):\n",
    "        axes[i].plot(errors[:, i], label=f'Error ({col})', alpha=0.7, color='red')\n",
    "        axes[i].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        axes[i].set_title(f'Prediction Error for {col}')\n",
    "        axes[i].set_xlabel('Time Steps')\n",
    "        axes[i].set_ylabel('Error')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_importance(feature_names, model_weights=None):\n",
    "    \"\"\"\n",
    "    Plot feature importance if available.\n",
    "    \n",
    "    Args:\n",
    "        feature_names: List of feature names\n",
    "        model_weights: Model weights (if available)\n",
    "    \"\"\"\n",
    "    if model_weights is None:\n",
    "        print(\"Feature importance not available for LSTM models.\")\n",
    "        return\n",
    "    \n",
    "    # This is a placeholder - LSTM feature importance is complex\n",
    "    # In practice, you might use permutation importance or SHAP values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(feature_names)), model_weights)\n",
    "    plt.title('Feature Importance (Placeholder)')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_summary_report(results, model_history=None):\n",
    "    \"\"\"\n",
    "    Create a summary report of model performance.\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from evaluate_model_performance\n",
    "        model_history: Training history (optional)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nMSE Scores:\")\n",
    "    for col, mse in results['mse_scores'].items():\n",
    "        print(f\"  {col}: {mse:.4f}\")\n",
    "    \n",
    "    # Calculate overall performance\n",
    "    avg_mse = np.mean(list(results['mse_scores'].values()))\n",
    "    print(f\"\\nAverage MSE: {avg_mse:.4f}\")\n",
    "    \n",
    "    if model_history is not None:\n",
    "        final_train_loss = model_history.history['loss'][-1]\n",
    "        final_val_loss = model_history.history['val_loss'][-1]\n",
    "        print(f\"\\nFinal Training Loss: {final_train_loss:.4f}\")\n",
    "        print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(\"  - Type: LSTM (Long Short-Term Memory)\")\n",
    "    print(\"  - Input Shape: (timesteps, features)\")\n",
    "    print(\"  - Output: Multi-output regression\")\n",
    "    print(\"  - Targets: Open, High, Low, Close, Volume\")\n",
    "    \n",
    "    print(\"\\nData Information:\")\n",
    "    print(f\"  - Training samples: {len(results['actual'])}\")\n",
    "    print(f\"  - Features per sample: {results['predictions'].shape[1]}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional visualizations and analysis\n",
    "if 'btc_data' in locals() and btc_data is not None:\n",
    "    print(\"Plotting original price data...\")\n",
    "    plot_price_data(btc_data, \"Bitcoin Price Data (2021)\")\n",
    "    \n",
    "if 'results' in locals() and results is not None:\n",
    "    print(\"\\nPlotting prediction errors...\")\n",
    "    plot_prediction_errors(results['predictions'], results['actual'], results['target_columns'])\n",
    "    \n",
    "    print(\"\\nGenerating summary report...\")\n",
    "    if 'history' in locals():\n",
    "        create_summary_report(results, history)\n",
    "    else:\n",
    "        create_summary_report(results)\n",
    "else:\n",
    "    print(\"Data or results not available for additional visualizations.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
